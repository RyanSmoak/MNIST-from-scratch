{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNTMTtxP/y7c54luA61Kqgs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RyanSmoak/MNIST-from-scratch/blob/main/MNIST_full.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scipy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tqy94gjaZSjO",
        "outputId": "14cf6ff7-1a19-475c-af9b-0642e360516b"
      },
      "execution_count": 195,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (1.13.1)\n",
            "Requirement already satisfied: numpy<2.3,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from scipy) (1.26.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 196,
      "metadata": {
        "id": "DivVQB0D5Rha"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from scipy import signal"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Conv2DLayer:\n",
        "  '''\n",
        "    This class creates the convolutional layer, the most basic and important layer in our CNN.\n",
        "  '''\n",
        "  def __init__(self, input_shape, filter_shape, stride, padding):\n",
        "    '''\n",
        "      This function initializes the convolutional layer.\n",
        "      Inputs:\n",
        "        - input_shape: The shape of the input structure (depth, height, width)\n",
        "        - filter_shape: The shape of the filter (num_filters, depth, height, width)\n",
        "        - stride: The number of steps the filter takes at each iteration\n",
        "        - padding: The amount of zero padding to be added to the input structure\n",
        "    '''\n",
        "    self.input_shape = input_shape\n",
        "    self.input_depth = input_shape[0]\n",
        "\n",
        "    self.num_filters = filter_shape[0]\n",
        "    self.filter_size = filter_shape[2]\n",
        "    self.stride = stride\n",
        "    self.padding = padding\n",
        "\n",
        "    #self.output_shape = (depth, input_height - filter_size + 1, input_width - filter_size + 1)\n",
        "    fan_in = input_shape[0] * filter_shape[2] * filter_shape[2]\n",
        "    self.filter_shape = filter_shape\n",
        "    self.filters = np.random.randn(filter_shape[0], input_shape[0], filter_shape[2], filter_shape[2])/np.sqrt(fan_in/2)\n",
        "    self.biases = np.zeros(filter_shape[0])\n",
        "\n",
        "  def zero_pad(self, input_data):\n",
        "    '''\n",
        "    This function pads the input with zeros to a certain degree.\n",
        "    Input:\n",
        "      - input_data: 2D array (H_1 x W_1)\n",
        "    Output:\n",
        "      - padded_input: 2D  padded array (H_1+padding X W_2+padding)\n",
        "    '''\n",
        "\n",
        "    batch_size, D_1, H_1, W_1 = input_data.shape\n",
        "    #create a padded array of zeros\n",
        "    padded_input = np.zeros((batch_size, D_1, H_1 + (2 * self.padding), W_1 + (2 * self.padding)))\n",
        "    #copy the input structure into the centre of the padded array\n",
        "    padded_input[:, :, self.padding:self.padding + H_1, self.padding:self.padding + W_1] = input_data\n",
        "\n",
        "    return padded_input\n",
        "\n",
        "  def stride_fun(self, input, h, w, filter_size):\n",
        "    '''\n",
        "    This function is meant to slide the filter along the structure volume a certain number of steps a\n",
        "    at each iteration.\n",
        "    Inputs:\n",
        "      - h: The height of the structure\n",
        "      - w: The width of the structure\n",
        "      - filter_size: The width and/or the height of the filter\n",
        "    Output:\n",
        "      - The input slice at a given iteration\n",
        "    '''\n",
        "    stride = self.stride\n",
        "     # Calculate the top-left corner of the current window\n",
        "    h_start = h * stride\n",
        "    w_start = w * stride\n",
        "\n",
        "    F_w = filter_size\n",
        "    F_h = filter_size\n",
        "    # Extract and return the input slice\n",
        "    return input[:, h_start:h_start + F_h, w_start:w_start + F_w]\n",
        "\n",
        "  def forward(self, input_data):\n",
        "    '''\n",
        "    This function does the actual convolution process that we described ealier in this cookbook.\n",
        "    '''\n",
        "    self.input_data = input_data\n",
        "    stride_num = self.stride\n",
        "    padding = self.padding\n",
        "    filter_size = self.filter_size\n",
        "    filters = self.filters\n",
        "    biases = self.biases\n",
        "\n",
        "    (C_in, H_in, W_in) = self.input_shape\n",
        "    batch_size = input_data.shape[0]\n",
        "    (num_filters, C_in_filter, F_h, F_w) = self.filter_shape\n",
        "\n",
        "    H_out = int(np.floor((H_in + 2*padding - F_h) / stride_num + 1))\n",
        "    W_out = int(np.floor((W_in + 2*padding - F_w) / stride_num + 1))\n",
        "\n",
        "    # Initialize the output tensor\n",
        "    output = np.zeros((batch_size, num_filters, H_out, W_out))\n",
        "\n",
        "    self.padded_input = self.zero_pad(input_data)\n",
        "\n",
        "    # Perform convolution\n",
        "    for b in range(batch_size):\n",
        "      for n in range(num_filters):  # Loop over each filter\n",
        "          for h in range(H_out):  # Loop over output height\n",
        "              for w in range(W_out):  # Loop over output width\n",
        "                  input_slice = self.stride_fun(self.padded_input[b], h, w, filter_size)\n",
        "\n",
        "                  # Perform dot product\n",
        "                  output[b, n, h, w] = np.sum(input_slice * filters[n]) + biases[n]\n",
        "\n",
        "    return output\n",
        "\n",
        "  def backward(self, output_gradient, optimizer):\n",
        "    '''\n",
        "    This function performs the backward pass to the network that will\n",
        "    '''\n",
        "    filters_gradient= np.zeros(self.filter_shape)\n",
        "    input_gradient = np.zeros(self.padded_input.shape)\n",
        "    bias_gradient = np.mean(output_gradient, axis=(0,2,3))\n",
        "\n",
        "    #compute the gradients\n",
        "    for b in range(self.input_data.shape[0]):\n",
        "      for i in range (self.num_filters):\n",
        "        for j in range (self. input_depth):\n",
        "          filters_gradient[i, j] += signal.correlate2d(self.input_data[b,j], output_gradient[b,i], \"valid\")\n",
        "          input_gradient[b,j] += signal.convolve2d(output_gradient[b,i], self.filters[i, j], \"full\")\n",
        "\n",
        "    self.filters, self.biases = optimizer.update(\n",
        "        self.filters, self.biases, filters_gradient, bias_gradient\n",
        "    )\n",
        "\n",
        "    return input_gradient"
      ],
      "metadata": {
        "id": "EQ83YnUv5a18"
      },
      "execution_count": 197,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MaxPool2D():\n",
        "  def __init__(self, pool_size, stride, padding):\n",
        "    self.pool_size = pool_size\n",
        "    self.stride = stride\n",
        "    self.padding = padding\n",
        "    self.input_shape = None\n",
        "    self.input_map = None\n",
        "\n",
        "  def forward(self, feature_map):\n",
        "    '''\n",
        "    This function is meant to act as the pooling layer after a Conv2D layer\n",
        "    Inputs:\n",
        "      feature_map: This is the output of the Conv2D layer\n",
        "      pool_size: This is the size of the pooling filter\n",
        "      stride: This the steps to be taken by the filter\n",
        "      padding: This is the amount of zero padding to be added\n",
        "    Output:\n",
        "      output: This is the input structure with reduced spatial dimanesions of the pooling layer\n",
        "    '''\n",
        "    #account for any padding that may be added\n",
        "\n",
        "    self.input_shape = feature_map.shape\n",
        "    self.input_map = feature_map\n",
        "\n",
        "    if self.padding > 0:\n",
        "          feature_map = np.pad(feature_map,\n",
        "                        ((0,0), (0,0), (self.padding, self.padding), (self.padding, self.padding)),\n",
        "                        mode='constant',\n",
        "                        constant_values=0)\n",
        "\n",
        "    #Get the shapes for the input and the pooling filter\n",
        "    batch_size, channels, H_in, W_in = feature_map.shape\n",
        "    pool_h, pool_w = self.pool_size\n",
        "\n",
        "    #calculate the output size\n",
        "    W_out = (W_in - pool_w) // self.stride + 1\n",
        "    H_out = (H_in - pool_h) // self.stride + 1\n",
        "\n",
        "    H_out = max(1, H_out)\n",
        "    W_out = max(1, W_out)\n",
        "\n",
        "    self.max_indices = np.zeros_like(feature_map)\n",
        "\n",
        "    #initialize the output\n",
        "    output_map = np.zeros((batch_size, channels, H_out, W_out))\n",
        "\n",
        "    #create the window and sliding\n",
        "    for b in range(batch_size):\n",
        "      for c in range(channels):\n",
        "        for i in range(H_out):\n",
        "          for j in range(W_out):\n",
        "            #slide the window\n",
        "            window = feature_map[b, c, i*self.stride : i*self.stride+pool_h,\n",
        "                                j*self.stride : j*self.stride+pool_w]\n",
        "            #perform max pooling\n",
        "            output_map[b, c, i, j] = np.max(window)\n",
        "            max_idx = np.unravel_index(np.argmax(window), window.shape)\n",
        "            self.max_indices[b, c, i+max_idx[0], j+max_idx[1]] = 1\n",
        "\n",
        "\n",
        "    return output_map\n",
        "\n",
        "  def backward(self, dL_dOutput):\n",
        "    '''\n",
        "    This function is meant to propagate the feature map with repect to the input.\n",
        "    Basically give us our gradient the same shape as the input\n",
        "    Input:\n",
        "      dL_dOutput: Gradients passed in the backprop\n",
        "    '''\n",
        "    dL_dInput = np.zeros_like(self.input_map)\n",
        "    batch_size, channels, out_height, out_width = dL_dOutput.shape\n",
        "\n",
        "    for b in range(batch_size):\n",
        "        for c in range(channels):\n",
        "            for i in range(out_height):\n",
        "                for j in range(out_width):\n",
        "                    # Find the corresponding window in the input\n",
        "                    window_start_i = i * self.stride\n",
        "                    window_start_j = j * self.stride\n",
        "                    window_end_i = window_start_i + self.pool_size[0]\n",
        "                    window_end_j = window_start_j + self.pool_size[0]\n",
        "\n",
        "                    # Only the position of the max value gets the gradient\n",
        "                    max_mask = self.max_indices[b, c, window_start_i:window_end_i, window_start_j:window_end_j]\n",
        "                    dL_dInput[b, c, window_start_i:window_end_i, window_start_j:window_end_j] += dL_dOutput[b, c, i, j] * max_mask\n",
        "\n",
        "    return dL_dInput"
      ],
      "metadata": {
        "id": "sz56EPGU5dDx"
      },
      "execution_count": 198,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PReLU():\n",
        "  def __init__(self, alpha=0.01):\n",
        "    '''\n",
        "      This function initializes the PReLU activation function. Seriously, initialize this stuff, makes life so much easier.\n",
        "    '''\n",
        "    self.alpha = alpha\n",
        "    self.alpha_grad = None\n",
        "    self.prelu_input = None\n",
        "\n",
        "  def forward(self, prev_layer_input):\n",
        "    '''\n",
        "      This function does the forward pass of the PReLU activation function.\n",
        "      Input:\n",
        "        prev_layer_input: The input from the previous layer, be it the conv2D or the FC layer.\n",
        "      Output:\n",
        "        The output of the PReLU activation function.\n",
        "    '''\n",
        "    #storing the input with self to use in the backprop function\n",
        "    self.prelu_input = prev_layer_input\n",
        "    return np.where(prev_layer_input > 0, prev_layer_input, self.alpha * prev_layer_input)\n",
        "\n",
        "  def backward(self, dy):\n",
        "    '''\n",
        "      This function does the backward pass of the PReLU activation function.\n",
        "      Input:\n",
        "        dy: The gradient of the loss function with respect to the output of the PReLU activation function.\n",
        "      Output:\n",
        "        dx: The gradient of the loss function with respect to the input of the PReLU activation function.\n",
        "    '''\n",
        "    if self.prelu_input is None:\n",
        "      raise ValueError(\"Input to PReLU activation function has not been computed yet.\")\n",
        "\n",
        "    # Gradient of the activation with respect to the input\n",
        "    dx = np.where(self.prelu_input > 0, dy, self.alpha * dy)\n",
        "\n",
        "    # Gradient of alpha: sum of the gradients where input <= 0\n",
        "    self.alpha_grad = np.sum(dy * self.prelu_input * (self.prelu_input <= 0))\n",
        "\n",
        "    return dx\n",
        "\n",
        "class Softmax():\n",
        "  def __init__(self):\n",
        "    self.output = None\n",
        "\n",
        "  def softmax(self, logits):\n",
        "    '''\n",
        "    This function is compute the class probabilites for a certain data point\n",
        "    using softmax activation function\n",
        "    Input:\n",
        "      x: The input array of size (batch_size, num_classes)\n",
        "    Output:\n",
        "      The class probabilites for the data point\n",
        "    '''\n",
        "    self.logits = logits\n",
        "    exp_x = np.exp(self.logits - np.max(self.logits, axis=0, keepdims=True))\n",
        "    return exp_x / np.sum(exp_x, axis=0, keepdims=True)\n"
      ],
      "metadata": {
        "id": "ALmGaGBw5g7r"
      },
      "execution_count": 199,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SGD_NAG:\n",
        "    def __init__(self, learning_rate, momentum):\n",
        "        '''\n",
        "        Always have an __init__ function in your class, seriously, do it.\n",
        "        Inputs:\n",
        "          learning_rate: The step size for the optimization.\n",
        "          momentum: The momentum coefficient.\n",
        "        '''\n",
        "\n",
        "        self.learning_rate = learning_rate\n",
        "        self.momentum = momentum\n",
        "        self.velocity_filters = None\n",
        "        self.velocity_biases = None\n",
        "\n",
        "    def update(self, filters, biases, filters_grads, biases_grads):\n",
        "        '''\n",
        "        After calculating the gradients in the backpass, this function\n",
        "        will update the parameters using Nesterov Accelerated Gradient.\n",
        "        Inputs:\n",
        "          filters: The current filters for the convLayer\n",
        "          biases: The biases for the convLayer\n",
        "          filters_grads: The gradients of the filters\n",
        "          biases_grads: The gradients of the biases\n",
        "        Output:\n",
        "          Updated filters and biases.\n",
        "        '''\n",
        "        self.filters = filters\n",
        "        self.biases = biases\n",
        "        #Initalize the filter and biases velocity\n",
        "        if self.velocity_filters is None:\n",
        "            self.velocity_filters = np.zeros_like(filters)\n",
        "        if self. velocity_biases is None:\n",
        "            self.velocity_biases = np.zeros_like(biases)\n",
        "\n",
        "        #lookahead for the filters and biases\n",
        "        lookahead_filters = self.filters - self.momentum * self.velocity_filters\n",
        "        lookahead_biases = self.biases - self.momentum * self.velocity_biases\n",
        "\n",
        "        #update velocities\n",
        "        self.velocity_filters = self.momentum * self.velocity_filters - self.learning_rate * filters_grads\n",
        "        self.velocity_biases = self.momentum * self.velocity_biases - self.learning_rate * biases_grads\n",
        "\n",
        "        #update parameters\n",
        "        updated_filters = lookahead_filters + self.velocity_filters + self.momentum * (self.velocity_filters - self.momentum * self.velocity_filters)\n",
        "        updated_biases = lookahead_biases + self.velocity_biases + self.momentum * (self.velocity_biases - self.momentum * self.velocity_biases)\n",
        "\n",
        "        return updated_filters, updated_biases"
      ],
      "metadata": {
        "id": "IvwCf5OW5m0R"
      },
      "execution_count": 200,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Flatten:\n",
        "  def __init__(self):\n",
        "    '''\n",
        "    An initialization function for the flatten layer.\n",
        "    I think by now you should know what I'm going to say for this\n",
        "    '''\n",
        "    self.input_shape = None #to store the original shape for unflatening\n",
        "\n",
        "  def flatten(self, input_tensor):\n",
        "    '''\n",
        "    Reshape the input tensor into a 1D vector.\n",
        "    Input:\n",
        "      input_tensor: The input tensor to be flattened.\n",
        "    Output:\n",
        "      The flattened tensor.\n",
        "    '''\n",
        "    self.input_shape = input_tensor.shape\n",
        "    return input_tensor.reshape(-1)\n",
        "\n",
        "  def unflatten(self, output_gradient):\n",
        "    '''\n",
        "    Reshape the output gradient from the FC layer into original shape\n",
        "    Input:\n",
        "      output_gradient: The output gradient from the FC layer.\n",
        "    Output:\n",
        "      The unflattened gradient.\n",
        "    '''\n",
        "    return output_gradient.reshape(self.input_shape)"
      ],
      "metadata": {
        "id": "jJdgW4HW5y31"
      },
      "execution_count": 201,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FullyConnectedLayer:\n",
        "    def __init__(self, input_neurons_num, output_neurons_num, activation_function, activation_derivative):\n",
        "        '''\n",
        "        Initialize the fully connected layer. No comment!\n",
        "        Input:\n",
        "          input_neurons_num: Number of input neurons.\n",
        "          output_neurons_num: Number of output neurons.\n",
        "          activation_function: The activation function to apply.\n",
        "          activation_derivative: The derivative of the activation function.\n",
        "        '''\n",
        "        self.input_size = input_neurons_num\n",
        "        self.output_size = output_neurons_num\n",
        "        self.activation_function_instance = activation_function\n",
        "        self.activation_derivative_instance = activation_derivative\n",
        "\n",
        "        # Initialize weights and biases with He initialization\n",
        "        self.weights = np.random.randn(output_neurons_num, input_neurons_num) * np.sqrt(2 / input_neurons_num)\n",
        "        self.biases = np.zeros((output_neurons_num, 1))\n",
        "        self.z = None\n",
        "        self.activated_output = None\n",
        "\n",
        "    def forward(self, input_data):\n",
        "        '''\n",
        "        Perform the forward pass through the FC layer.\n",
        "        Input:\n",
        "          input_data: Input data of shape (input_neurons_num, batch_size).\n",
        "        Output:\n",
        "          Output data after applying the activation function.\n",
        "        '''\n",
        "        input_data = input_data.reshape(self.input_size, -1)\n",
        "        self.input = input_data  # Store input for use in backward pass\n",
        "        self.z = np.dot(self.weights, input_data) + self.biases\n",
        "\n",
        "        if self.activation_function_instance is not None:\n",
        "          if isinstance(self.activation_function_instance, PReLU):\n",
        "            self.activated_output = self.activation_function_instance.forward(self.z)\n",
        "          else:\n",
        "              self.activated_output = self.activation_function_instance(self.z)\n",
        "          return self.activated_output\n",
        "        else:\n",
        "          self.activated_output = self.z\n",
        "\n",
        "          return self.activated_output\n",
        "\n",
        "    def backward(self, output_gradient, learning_rate):\n",
        "        '''\n",
        "        Perform the backward pass through the FC layer.\n",
        "        Input:\n",
        "          output_gradient: Gradient of the loss w.r.t. the output (a) of this layer.\n",
        "          learning_rate: Learning rate for weight updates.\n",
        "        Output:\n",
        "          dx Gradient of the loss w.r.t. the input (x) of this layer.\n",
        "        '''\n",
        "\n",
        "        # Compute gradient w.r.t. z\n",
        "        if self.activation_derivative_instance is not None:\n",
        "          if isinstance(self.activation_function_instance, PReLU):\n",
        "              dz = output_gradient * self.activation_derivative_instance.backward(self.activated_output)\n",
        "          else:\n",
        "              dz = output_gradient * self.activation_derivative_instance(self.activated_output)\n",
        "\n",
        "          #dz = output_gradient * self.activation_derivative(self.activated_output)  # Element-wise multiplication\n",
        "        else: dz = output_gradient\n",
        "\n",
        "        # Compute gradients\n",
        "        dw = np.dot(dz, self.input.T) / dz.shape[1]  # Weight gradient\n",
        "        db = np.sum(dz, axis=1, keepdims=True) / dz.shape[1]  # Bias gradient\n",
        "\n",
        "        # Update weights and biases\n",
        "        self.weights -= learning_rate * dw\n",
        "        self.biases -= learning_rate * db  # Broadcasting now works correctly\n",
        "\n",
        "        # Compute input gradient\n",
        "        dx = np.dot(self.weights.T, dz)\n",
        "        return dx\n",
        "\n"
      ],
      "metadata": {
        "id": "PgbzgycL50Re"
      },
      "execution_count": 202,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class losses():\n",
        "  def __init__(self, y_true, y_pred):\n",
        "    '''\n",
        "    Initialize the loss function.\n",
        "    Input:\n",
        "      y_true: The true labels.\n",
        "      y_pred: The predicted labels.\n",
        "    '''\n",
        "    self.y_true = y_true\n",
        "    self.y_pred = y_pred\n",
        "\n",
        "  def binary_cross_entropy(self):\n",
        "    '''\n",
        "    Compute the binary cross-entropy loss.\n",
        "    Output:\n",
        "      The binary cross-entropy loss.\n",
        "    '''\n",
        "    return -(1/self.m) * np.sum(self.y_true * np.log(self.y_pred) + (1 - self.y_true) * np.log(1 - self.y_pred))\n",
        "\n",
        "\n",
        "  def cross_entropy(self):\n",
        "    '''\n",
        "    Compute the cross-entropy loss for multi-class classification.\n",
        "    Output:\n",
        "      The cross-entropy loss.\n",
        "    '''\n",
        "    batch_size = self.y_pred.shape[0]\n",
        "\n",
        "    # Ensure y_true is an integer array of class indices\n",
        "    y_true = self.y_true.astype(int)\n",
        "\n",
        "    if y_true.ndim > 1:\n",
        "      y_true = np.argmax(y_true, axis=1)\n",
        "\n",
        "    correct_class_probs = self.y_pred[np.arange(batch_size), y_true]\n",
        "    log_probs = np.log(correct_class_probs + 1e-15)\n",
        "\n",
        "    log_probs = log_probs[:, np.newaxis]\n",
        "\n",
        "    return -np.mean(np.sum(self.y_true * log_probs, axis =1))"
      ],
      "metadata": {
        "id": "c5WDtLu757zR"
      },
      "execution_count": 203,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BatchNormalization:\n",
        "  def __init__(self, input_size, epsilon=1e-5, momentum=0.9):\n",
        "    '''\n",
        "    I really don't know what else I can say about initilizations.\n",
        "    Input:\n",
        "      input_size: batch size\n",
        "      epsilon: a small positive number to avoid division by 0\n",
        "      momentum: momentum coefficient (friction) when using SGD with momentum\n",
        "    '''\n",
        "    self.gamma = np.ones((input_size, 1))\n",
        "    self.beta = np.zeros((input_size, 1))\n",
        "    self.epsilon = epsilon\n",
        "    self.momentum = momentum\n",
        "    self.running_mean = np.zeros((input_size, 1))\n",
        "    self.running_var = np.zeros((input_size, 1))\n",
        "\n",
        "  def forward(self, x, training=True):\n",
        "    '''\n",
        "    This is the forward pass for batch normalization\n",
        "    Input:\n",
        "      training: a boolean telling us whether the network should learn gamma and beta or not\n",
        "    Output:\n",
        "      The normalized output\n",
        "    '''\n",
        "    if training:\n",
        "        self.mean = np.mean(x, axis=1, keepdims=True)\n",
        "        self.var = np.var(x, axis=1, keepdims=True)\n",
        "        self.x_hat = (x - self.mean) / np.sqrt(self.var + self.epsilon)\n",
        "\n",
        "        # Update running estimates\n",
        "        self.running_mean = self.momentum * self.running_mean + (1 - self.momentum) * self.mean\n",
        "        self.running_var = self.momentum * self.running_var + (1 - self.momentum) * self.var\n",
        "    else:\n",
        "        # Use running estimates for inference\n",
        "        self.x_hat = (x - self.running_mean) / np.sqrt(self.running_var + self.epsilon)\n",
        "\n",
        "    self.out = self.gamma * self.x_hat + self.beta\n",
        "    return self.out\n",
        "\n",
        "  def backward(self, dout):\n",
        "    '''\n",
        "    This function is meant to 'undo' the effect of the normalization by backprop\n",
        "    so as to compute the gradient of x\n",
        "    Input:\n",
        "      dout: the gradient of a later layer in the network\n",
        "\n",
        "    Output:\n",
        "      dx: the gradient of x backpropagated through the batch normalization\n",
        "    '''\n",
        "    m = dout.shape[1]\n",
        "\n",
        "    # Gradients w.r.t. gamma and beta\n",
        "    self.dgamma = np.sum(dout * self.x_hat, axis=1, keepdims=True)\n",
        "    self.dbeta = np.sum(dout, axis=1, keepdims=True)\n",
        "\n",
        "    # Backprop through normalization\n",
        "    dx_hat = dout * self.gamma\n",
        "    dvar = np.sum(dx_hat * (self.input - self.mean) * -0.5 * (self.var + self.epsilon)**-1.5, axis=1, keepdims=True)\n",
        "    dmean = np.sum(dx_hat * -1 / np.sqrt(self.var + self.epsilon), axis=1, keepdims=True) + dvar * np.mean(-2 * (self.input - self.mean), axis=1, keepdims=True)\n",
        "\n",
        "    dx = dx_hat / np.sqrt(self.var + self.epsilon) + dvar * 2 * (self.input - self.mean) / m + dmean / m\n",
        "    return dx\n"
      ],
      "metadata": {
        "id": "DiJaP42F6Hs8"
      },
      "execution_count": 204,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Dropout:\n",
        "    def __init__(self, keep_prob=0.5):\n",
        "        self.keep_prob = keep_prob\n",
        "\n",
        "    def forward(self, x, training=True):\n",
        "        if training:\n",
        "            self.mask = (np.random.rand(*x.shape) < self.keep_prob) / self.keep_prob\n",
        "            return x * self.mask\n",
        "        else:\n",
        "            return x  # During inference, no dropout is applied\n",
        "\n",
        "    def backward(self, dout):\n",
        "        return dout * self.mask"
      ],
      "metadata": {
        "id": "-b4iYxYi6Pww"
      },
      "execution_count": 205,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class evaluation():\n",
        "  def __init__(self, y_true, y_pred):\n",
        "    '''\n",
        "    Initialize the evaluation metrics.\n",
        "    Input:\n",
        "      y_true: The true class labels.\n",
        "      y_pred: The predicted labels by our network.\n",
        "    '''\n",
        "    self.y_true = y_true\n",
        "    self.y_pred = y_pred\n",
        "\n",
        "  def accuracy(self):\n",
        "    '''\n",
        "    Compute the accuracy of the model.\n",
        "    Output:\n",
        "      The accuracy of the model.\n",
        "    '''\n",
        "    return np.mean(self.y_true == self.y_pred)\n",
        "\n",
        "  def confusion_matrix(self, num_classes):\n",
        "    '''\n",
        "    Create a confusion matrix to show the relationship between classes.\n",
        "    Output:\n",
        "      The confusion matrix.\n",
        "    '''\n",
        "    conf_matrix = np.zeros((num_classes, num_classes), dtype=int)\n",
        "    for true_label, pred_label in zip(self.y_true, self.y_pred):\n",
        "        conf_matrix[true_label, pred_label] += 1\n",
        "    return conf_matrix"
      ],
      "metadata": {
        "id": "JHXyObzE6QhD"
      },
      "execution_count": 206,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN():\n",
        "  def __init__(self):\n",
        "    self.conv1 = Conv2DLayer(input_shape=(1,28,28),\n",
        "                             filter_shape = (32,1,2,2),\n",
        "                             stride=1,\n",
        "                             padding=1)\n",
        "\n",
        "    self.pool1 = MaxPool2D(pool_size=(2,2), stride=2, padding=0)\n",
        "\n",
        "    self.conv2 = Conv2DLayer(input_shape=(32,14,14),\n",
        "                             filter_shape = (64,32,2,2),\n",
        "                             stride=1,\n",
        "                             padding=1)\n",
        "\n",
        "    self.pool2 = MaxPool2D(pool_size=(2,2), stride=2, padding=0)\n",
        "    self.flatten = Flatten()\n",
        "\n",
        "    self.prelu_fc1 = PReLU()\n",
        "\n",
        "    self.fc1 = FullyConnectedLayer(3136, 128,\n",
        "                                   activation_function= self.prelu_fc1.forward,\n",
        "                                   activation_derivative=self.prelu_fc1.backward)\n",
        "\n",
        "    self.dropout = Dropout(keep_prob=0.5)\n",
        "\n",
        "    self.fc2 = FullyConnectedLayer(128, 10,\n",
        "                                   activation_function= Softmax().softmax,\n",
        "                                   activation_derivative=None)\n",
        "\n",
        "    self.softmax = Softmax()\n",
        "    self.prelu1 = PReLU()\n",
        "    self.prelu2 = PReLU()\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = self.conv1.forward(x)\n",
        "    out = self.prelu1.forward(out)\n",
        "    out = self.pool1.forward(out)\n",
        "\n",
        "    out = self.conv2.forward(out)\n",
        "    out = self.prelu2.forward(out)\n",
        "    out = self.pool2.forward(out)\n",
        "\n",
        "    out = self.flatten.flatten(out)\n",
        "\n",
        "    out = self.fc1.forward(out)\n",
        "    out = self.prelu_fc1.forward(out)\n",
        "\n",
        "    out = self.dropout.forward(out, training=True)\n",
        "\n",
        "    out = self.fc2.forward(out)\n",
        "    out = self.softmax.softmax(out)\n",
        "\n",
        "    return out\n",
        "\n",
        "  def backward(self, d_loss, learning_rate, optimizer):\n",
        "    optimizer_conv1 = optimizer[0]\n",
        "    optimizer_conv2 = optimizer[1]\n",
        "    grad = d_loss\n",
        "\n",
        "    grad = self.fc2.backward(grad.T, learning_rate)\n",
        "    grad = self.dropout.backward(grad)\n",
        "\n",
        "    grad = self.prelu_fc1.backward(grad)\n",
        "    grad = self.fc1.backward(grad, learning_rate)\n",
        "    grad = self.flatten.unflatten(grad)\n",
        "\n",
        "    grad = self.pool2.backward(grad)\n",
        "    grad = self.prelu2.backward(grad)\n",
        "    grad = self.conv2.backward(grad, optimizer_conv2)\n",
        "\n",
        "    grad = self.pool1.backward(grad)\n",
        "    grad = self.prelu1.backward(grad)\n",
        "    grad = self.conv1.backward(grad, optimizer_conv1)\n",
        "\n",
        "    return grad"
      ],
      "metadata": {
        "id": "CG0l0DXN6fh8"
      },
      "execution_count": 207,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keras"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JEoxQ0Sv70Rq",
        "outputId": "d94428fa-51a8-40ab-c7cf-3aa759c8f729"
      },
      "execution_count": 208,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (3.5.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from keras) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from keras) (1.26.4)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras) (0.0.8)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from keras) (3.12.1)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras) (0.13.1)\n",
            "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.10/dist-packages (from keras) (0.4.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from keras) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from optree->keras) (4.12.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.datasets import mnist\n",
        "\n",
        "def load_mnist_data():\n",
        "    \"\"\"\n",
        "    Load MNIST dataset and preprocess it.\n",
        "    :return: Tuple of (train_data, train_labels, test_data, test_labels)\n",
        "    \"\"\"\n",
        "    # Load MNIST data from tensorflow.keras\n",
        "    (train_data, train_labels), (test_data, test_labels) = mnist.load_data()\n",
        "\n",
        "    # Normalize the data to be in range [0, 1]\n",
        "    train_data = train_data.astype(np.float32) / 255.0\n",
        "    test_data = test_data.astype(np.float32) / 255.0\n",
        "\n",
        "    # Reshape the data to have shape (batch_size, channels, height, width)\n",
        "    train_data = train_data.reshape(-1, 1, 28, 28)\n",
        "    test_data = test_data.reshape(-1, 1, 28, 28)\n",
        "\n",
        "    # One-hot encode the labels\n",
        "    train_labels = np.eye(10)[train_labels]  # One-hot encoding for 10 classes\n",
        "    test_labels = np.eye(10)[test_labels]\n",
        "\n",
        "    return train_data, train_labels, test_data, test_labels"
      ],
      "metadata": {
        "id": "qPDYsBrw7ulx"
      },
      "execution_count": 209,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def train(model, train_data, train_labels, epochs, batch_size, learning_rate, optimizer):\n",
        "    \"\"\"\n",
        "    Train the CNN model using the provided training data.\n",
        "\n",
        "    :param model: The CNN model instance.\n",
        "    :param train_data: Training data (images).\n",
        "    :param train_labels: Training labels (one-hot encoded).\n",
        "    :param epochs: Number of epochs to train.\n",
        "    :param batch_size: Size of each training batch.\n",
        "    :param learning_rate: Learning rate for parameter updates.\n",
        "    :param optimizer: Optimizer instance (e.g., SGD or SGD with Nesterov momentum).\n",
        "    \"\"\"\n",
        "    num_samples = train_data.shape[0]\n",
        "    num_batches = num_samples // batch_size\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        epoch_loss = 0\n",
        "        epoch_accuracy = 0\n",
        "\n",
        "        # Shuffle data at the beginning of each epoch\n",
        "        perm = np.random.permutation(num_samples)\n",
        "        train_data = train_data[perm]\n",
        "        train_labels = train_labels[perm]\n",
        "\n",
        "        for i in range(num_batches):\n",
        "            # Get the current batch\n",
        "            batch_start = i * batch_size\n",
        "            batch_end = (i + 1) * batch_size\n",
        "            x_batch = train_data[batch_start:batch_end]\n",
        "            y_batch = train_labels[batch_start:batch_end]\n",
        "\n",
        "            # Forward pass: compute output and loss\n",
        "            y_pred = model.forward(x_batch).transpose()\n",
        "            loss_instance = losses(y_batch, y_pred)\n",
        "            loss = loss_instance.cross_entropy()\n",
        "            epoch_loss += loss\n",
        "\n",
        "            # Compute accuracy for this batch\n",
        "            batch_accuracy = np.mean(np.argmax(y_pred, axis=1) == np.argmax(y_batch, axis=1))\n",
        "            epoch_accuracy += batch_accuracy\n",
        "\n",
        "            # Backward pass: compute gradients and update weights\n",
        "            d_loss = y_pred - y_batch  # For cross-entropy + softmax\n",
        "            model.backward(d_loss, learning_rate, optimizer)\n",
        "\n",
        "\n",
        "        # Average loss and accuracy for the epoch\n",
        "        epoch_loss /= num_batches\n",
        "        epoch_accuracy /= num_batches\n",
        "\n",
        "        print(f\"Epoch {epoch + 1}/{epochs}, Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.4f}\")\n",
        "\n",
        "# Example of training loop setup\n",
        "if __name__ == \"__main__\":\n",
        "    # Assuming you have a function to load MNIST dataset\n",
        "    train_data, train_labels, test_data, test_labels = load_mnist_data()\n",
        "\n",
        "    # Initialize the model and optimizer\n",
        "    cnn_model = CNN()\n",
        "    optimizer_conv1 = SGD_NAG(learning_rate=0.01, momentum=0.9)\n",
        "    optimizer_conv2 = SGD_NAG(learning_rate=0.01, momentum=0.9)\n",
        "\n",
        "    # Train the model\n",
        "    train(cnn_model, train_data, train_labels, epochs=10, batch_size=64,\n",
        "          learning_rate=0.01,\n",
        "          optimizer=[optimizer_conv1, optimizer_conv2])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_jtEapYr6nMI",
        "outputId": "614ebd8d-2b17-4f28-ed99-73d9dda000f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-205-ab46ca81d749>:8: RuntimeWarning: overflow encountered in multiply\n",
            "  return x * self.mask\n",
            "<ipython-input-205-ab46ca81d749>:8: RuntimeWarning: invalid value encountered in multiply\n",
            "  return x * self.mask\n",
            "<ipython-input-199-6a632b5c3102>:55: RuntimeWarning: invalid value encountered in subtract\n",
            "  exp_x = np.exp(self.logits - np.max(self.logits, axis=0, keepdims=True))\n",
            "<ipython-input-199-6a632b5c3102>:37: RuntimeWarning: overflow encountered in multiply\n",
            "  self.alpha_grad = np.sum(dy * self.prelu_input * (self.prelu_input <= 0))\n",
            "<ipython-input-199-6a632b5c3102>:37: RuntimeWarning: invalid value encountered in multiply\n",
            "  self.alpha_grad = np.sum(dy * self.prelu_input * (self.prelu_input <= 0))\n"
          ]
        }
      ]
    }
  ]
}