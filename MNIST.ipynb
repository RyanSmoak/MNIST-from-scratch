{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMBXtR8wsj70yNfFzbJ2u+O",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RyanSmoak/MNIST-from-scratch/blob/main/MNIST.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#MNIST: Handwritten digit recognition\n",
        "\n",
        "In this cookbook, we will work on building a CNN from scratch to identify and recognise handwritten digits (the common MNIST problem). This mini project is limited to only recognition and not other computer vision tasks. Some key notes before starting:\n",
        "1. All layers in this cookbook will be implemented from scratch. That is:\n",
        "  - Convolutional layer\n",
        "  - Fully connneted layer (dense layer)\n",
        "  - Pooling layer\n",
        "  - Flattening layer\n",
        "2. Activation functions (ReLU/PReLU/Softmax) and loss functions (cross-entropy) will be implemented from scratch.\n",
        "3. The optimization technique used (SGD + Nestrov Momentum) will be implemented from scractch.\n",
        "4. Before each code cell, there will a markdown cell that includes a detailed explanation of what the code does. Some (NOT all) will include a brief mathematical explanation.\n",
        "\n",
        "Also be aware that the code in the code cells are just skeletons. They have not been debugged or modified for batch processing or broadcasting. They are just implementations to explain the content. The correct and modified code exists in separate files on GitHub\n",
        "\n",
        "That being said, we can get started."
      ],
      "metadata": {
        "id": "gtbcFyuDr1nq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Importing dependancies"
      ],
      "metadata": {
        "id": "4ZT7pZNXAbTF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Just like any project, we have to import some quick dependancies/libraries. These dependancies include:\n",
        "1. numpy - Honestly, if you know anything about AI/ML, you should not be shocked that this library is here. It's everywhere in AI/ML, because it allows to manipulate the most basic data in AI, numbers.\n",
        "2.  signal - Usually used in signal processing, but this library has some useful functions for our use. These functions are the cross-correlation and convolution functions. To be disccused later."
      ],
      "metadata": {
        "id": "NVft2KFcrY4w"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i8kF4DbMpsPZ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from scipy import signal"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Convolutional (Conv2D) Layer**"
      ],
      "metadata": {
        "id": "8zc3WJscAXde"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Forward Prop**"
      ],
      "metadata": {
        "id": "T4JglOqt7y61"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "The first thing is of course to work write our convoluation layer since it is the most important thing. Some quick explanation of what convolution/convLayer is, first. I will be detailed but not go too deep into it. We'll start with the forward pass part of the network before working on the backprop.\n",
        "\n",
        "Firstly, the difference between a convulational layer and a standard fully connected (dense) layer. It's quite simple really. A convolutional layer simply preserves the spatial structure of it's input. This is really important since we want to preserve key parts of an image (which is what we deal in with CNNs).\n",
        ">Example: *A $32 x 32 x 3$ image would be strecthed to a $3074x1$ matrix for a fully connected layer. In a conv layer, it would remain as is.*\n",
        "\n",
        "The actual convolution process is by convolving the image with a filter (AKA a kernel, receptive field) of equal depth i.e *slide the filter matrix over the image and compute it's dot product.* The result of this operation is called an activation map.\n",
        "\n",
        "The convolution operation itself **looks** as follows:\n",
        "> $$Y_i = B_i \\quad+\\quad \\sum_{j=1}^{n}X_j *K_{ij},\\qquad i = 1...d$$\n",
        "Where $Y$ is the output, $B$ is the bias, $X$ is the input matrix, $K$ is the kernel/filter/receptive field. The depth of the output is same as that of the input structure.\n",
        ">>Also, note that the asterik ($*$) is not element-wise multiplication of the matrices. It is the convolution process which is a dot product of a part of the input matrix and the filter.\n",
        "\n",
        "Above is the basic convolution process. There are 2 more things we need to deal with before this becomes a fully functioning convolutional layer. We need to think about:\n",
        "1. Stride (S): This is the number of steps the filter takes while sliding across the input structure\n",
        "```\n",
        "h, w = 6\n",
        "stride = 2\n",
        "#Calculate the top-left corner of the current window\n",
        "h_start = h * stride\n",
        "w_start = w * stride\n",
        "F_h, F_w = 2\n",
        "#Extract the input slice\n",
        "input_slice = input[:, h_start:h_start + F_h, w_start:w_start + F_w]\n",
        "```\n",
        "2. Padding (P): This is a layer added all around the input structure at the boarders to maintain spatial integrity and prevent data loss along the edges. We will use zero padding for this project.\n",
        "```\n",
        "D_1, H_1, W_1 = input.shape\n",
        "#create a padded array of zeros\n",
        "padded_input = np.zeros((D_1, H_1 + 2 * self.padding, W_1 + 2 * self.padding))\n",
        "#copy the input structure into the centre of the padded array\n",
        "padded_input[:, self.padding:self.padding + H_1, self.padding:self.padding + W_1] = input\n",
        "```\n",
        "\n",
        "Taking into account our new hyperparameters, we have a new formula for the getting the output matrix volume:\n",
        "$$\\left(\\frac{N-F+2P}{S}\\right) + 1$$\n",
        "> Remember, the depth is not altered. The output depth is the same as the structure depth.\n",
        "\n",
        "\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "YFBJHn_R0Yvj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **The filters**\n",
        "\n",
        "We need to talk about this filters a little cause it may not be as obvious what they are. The simplest way to explain them is as weights. Actually, that's exactly what they are. Feature weights. In standard ML, we are used to each input feature being associated with a certain weight. Well, that's what we're doing but now each input feature ($d$-th depth of an input image) is associated with weight matrix (filter).\n",
        "\n",
        "Just like any ML/DL model, we need to initialise the weights beforehand. That is, before all the work of building up the model, we need to have some random weights to start with so the model can learn and update them.\n",
        "\n",
        "How do we do this? The go-to method for small networks is just random small random numbers. This is good for small networks since learning isn't deep, so while using backprop, our gradients are less likely to reach 0 (they may tend to 0, but not get to 0). For larger, more sophisticated networks, we use better methods.\n",
        "\n",
        "In this cookbook, we will use one of these better methods: He weight initialization. Why? Just cause we can😁. The basic idea of He initialization is initializing weights with values drawn from a normal distribution, but with a variance that is scales according to the number of input neurons for each weight. Of course it has a formula and of course I'll add it here cause it's a really fun one:\n",
        "> $$w \\approx \\mathcal{N} \\left(0, \\frac{2}{n_{in}}\\right)$$\n",
        "> where:\n",
        "- $\\mathcal{N}$ is the normal (Gaussian) distribution with mean $0$ and variance $ \\frac{2}{n_{in}}$\n",
        "- $n_{in}$ is the number of input neurons to the current layer.\n",
        "\n",
        "The code for this generally looks like (in python):\n",
        "```\n",
        "input_depth, depth = 1\n",
        "filter_size = 3\n",
        "num_filters = 32\n",
        "\n",
        "fan_in = input_depth * filter_size * filter_size\n",
        "filter_shape = (depth, input_depth, filter_size, filter_size)\n",
        "filters = np.random.randn(num_filters, input_depth, filter_size, filter_size)/np.sqrt(fan_in/2)\n",
        "biases = np.zeros(num_filters)\n",
        "```\n",
        "\n",
        "I want to be fancy so, obviously I'd use this. There's a much simpler method known as Xavier initialization. Why didn't we use the easier one? Because the activation function we're using is PReLU, which would end up blowing up the gradients in the negative realm. We'll discuss this a bit deeper when we implement the activation function."
      ],
      "metadata": {
        "id": "oPAySidjorqj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **A quick BTW**\n",
        "\n",
        "I feel as if I should mention this early. There are MULTIPLE types of convolution layers. Some examples are:\n",
        "1. **Conv1D** - ConvLayers that work with 1-D data like time series or text. Majorly used in sequential data analysis and audio processing.\n",
        "2. **Conv2D** - ConvLayers that work with 2-D data like images. Input data usually 2 dimensions (height and width) with an extra 'dimension' (like channels). Used in image recognition and segmentation. This is what we'll be creating.\n",
        "3. **Conv3D** - ConvLayers that work with 3-D data. Input data usually has 3 dimensions (height, width, and depth) and an extra 'dimension' (like channels). Used in volumetric data processing and time-series data with spatial dimenesions (video processing).\n",
        "4. **Transposed convolution (Deconvolution)** - Used for upsampling the input. Works in the opposite manner as a normal convLayer by increasing the spatial dimensions of the input. Used in Generative Adversial Networks (GAN) and autoencoders.\n",
        "\n",
        "And so many more...\n",
        "\n",
        "We're building a Conv2D layer since we're working with images.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "u6pNuEGUJjV2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Forward Prop Summary**\n",
        "\n",
        "At this point, we have built the forward pass of our Conv Layer. A quick summary of what is going on there:\n",
        "\n",
        "> Even though it wasn't the first thing we talked about, the first thing is to  initialize our filters (weights) using He initialization. This step also includes initiliazing various parameters such as:\n",
        "- input shape\n",
        "- number of filters\n",
        "- stride\n",
        "- padding\n",
        "\n",
        "> After initialization, we've padded our input using zero padding and set a stride for our filter.\n",
        "\n",
        "> Calculate the output shape volume using: $\\left(\\frac{N - F+2P}{S}\\right)+1$\n",
        "\n",
        "> We initialize the output matrix with the calculated size\n",
        "\n",
        "> Perform the convolution process on the input structure to get and copy the results to the output matrix."
      ],
      "metadata": {
        "id": "uUjMFAZl4-2Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**BackProp**"
      ],
      "metadata": {
        "id": "p5bZj8UI8Fhv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The back pass, known as backprop isn't as complicated as most people think it is. Once you understand the fundamental concepts of backprop in a FC (dense) layer, you'll easily get for any other sort of layer. All we need to do for the backprop in a CNN is calculate the gradient of the loss w.r.t the filters, bias and input. Like I said, in this cookbook I will not be going too deep so we won't discuss loss functions. I'll keep it basic.\n",
        "\n",
        "For this cookbook, I'll write the formulas and give ONLY an intuition of why the work.\n",
        "\n",
        "Some notation first:\n",
        "- $E$ is the loss function. In our case, assume binary cross-entropy as our loss function\n",
        "- $Y_i$ is the output produced by the forward pass at any given iteration\n",
        "- $∂E$ is the error signal *i.e* the gradient of the loss function w.r.t $Y_i$\n",
        "- $K_{rot}$ is the filter rotated $180°$ about its centre\n",
        "- $K_{ij}$ is a cell of the filter at any given iteration\n",
        "- $B$ is the bias associated with forward pass at any iteration\n",
        "- $X_j = X_{region}[i,j]$ is the $K$ x $K$ patch of the input that the filter was applied to produce $Y[f,i,j]$\n",
        "- $conv$  is the convolution process\n",
        "\n",
        "1. Gradient w.r.t the filter\n",
        "> $dW = convolve(Input, dOutput)$\n",
        ">$$\\frac{∂E}{∂K_{ij}} = \\sum_i \\frac{∂E}{∂Y_i}\\quad\\cdot\\quad X_j$$\n",
        ">> In the backward pass, the gradient of the output w.r.t the filter is the 'reverse' of this sliding that is done in the forward pass. We convolve the error signal $dOutput$ $(∂E)$ with the input to compute $dW$.\n",
        "```\n",
        "  for f in range(self.num_filters):\n",
        "      for i in range(out_h):\n",
        "          for j in range(out_w):\n",
        "              region = input_padded[:,\n",
        "                                    i * self.stride:i * self.stride + self.filter_size,\n",
        "                                    j * self.stride:j * self.stride + self.filter_size]\n",
        "              dE_dK[f] += region * dE_dY[f, i, j]\n",
        "      dE_db[f] = np.sum(dE_dY[f])  # Sum over all positions for the bias gradient\n",
        "```\n",
        "\n",
        "2. Gradient w.r.t the bias\n",
        "> $dB = \\sum dOutput$\n",
        ">$$\\frac{∂E}{∂B_i} = \\sum \\frac{∂E}{∂Y_i} $$\n",
        ">> In the forward pass, the bias ($b$) is added to every output at any iteration, which means it directly contributes to the output. Since $b$ is added uniformly to all output positions, the gradient of the loss w.r.t the bias is the sum of all elements in the gradient of the loss w.r.t the output.\n",
        "```\n",
        "df_db = output_gradient\n",
        "```\n",
        "Since the gradient w.r.t the bias is the same as the output gradient, it can be passed as a parameter to the backprop function from the next layer.\n",
        "\n",
        "3. Gradient w.r.t the input\n",
        ">$dInput = reverse\\_convolve(dOuput, Filters)$\n",
        ">$$\\frac{∂E}{∂X_j} = \\sum_{i=i}^{n}  conv\\left(\\frac{∂E}{∂Y_i}, K_{rot}\\right) $$\n",
        ">> The input can only contribute to the output through the filter $K_{ij}$. We can use this relatinship to compute $\\frac{∂E}{∂X_j}$. The gradient is gotten by fully convolving our the filter $K_{rot}$ with tthe output gradient.\n",
        ">>> **NOTE:** The convolution here is full and not valid, like it was in the forward pass. This is because the gradient needs to cover all positions of the input including those that may have been padded.  \n",
        "```\n",
        "for i in range(out_h):\n",
        "  for j in range(out_w):\n",
        "      for f in range(self.num_filters):\n",
        "          region = dE_dX[:,\n",
        "                          i * self.stride:i * self.stride + self.filter_size,\n",
        "                          j * self.stride:j * self.stride + self.filter_size]\n",
        "          region += self.filters[f] * dE_dY[f, i, j]\n",
        "```\n",
        "\n",
        "Like I said, the markdowns will be detailed but I will not go too deep into the theory. For this case, I will not be including the derivation of these formulas in this cookbook (even though, they are very fun and interesting). Instead, we will use the intuition I've given above and the simplified formulas and implement them in code.\n",
        "\n",
        "After that, we can use another algorithm to update the weights. In this case, we'll use Adam's. To be discussed later.\n"
      ],
      "metadata": {
        "id": "yNQpgg3v8U8w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Full summary\n",
        "\n",
        "At this point, we have knowledge to develop a algortihm for a basic convolutional layer. First, a summary of what the conv layer is meant to do.\n",
        "\n",
        ">Accepts a volume of size $W_1*H_1*D_1$\n",
        "\n",
        ">Requires 4 hypermaters\n",
        "- Number of filters, $K$\n",
        "- Spatial extent of the filter, $F$\n",
        "- Stride, $S$\n",
        "- Amount of zero padding, $P$\n",
        "\n",
        ">Produce a volume of size $W_2*H_2*D_2$ where\n",
        "- $W_2 = \\left(\\frac{W_1 - F+2P}{S}\\right)+1$\n",
        "- $H_2 = \\left(\\frac{H_1 - F+2P}{S}\\right)+1$\n",
        "- $D_2 = K$\n",
        "\n",
        ">> **NOTE:** The depth is maintained. The width and height are computed equally by symmetry\n",
        "\n",
        ">With parameter sharing, it introduces $F.F.D_1$ weights per filter for a total of $(F.F.D_1).K$ weights and $K$ biases.\n",
        "\n",
        "> In he output's volume, the $d$-th depth slice is the result of performing a *valid* convoluton of the $d$-th filter over the input volume with a stride $S$ and then offset by the $d$-th bias.\n",
        "\n",
        ">> **NOTE:** The word valid has been italicised since it is one of 2 types of convolutions:\n",
        "1. Valid convolution - Where the kernel starts and stops at the boarder of the input structure\n",
        "2. Full convolution - The kernel starts multiplication as soon as the intersection between the filter and the input structure is not 0 i.e *there's at least one cell in the filter matrix that is inline with the input.* This type of convolution leads to a larger output, which means it is more computationally expensive.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "vY1M2tlRAijF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Actual Conv2D Layer\n",
        "\n",
        "Quick note: Wherever the word tensor has been used, just know that is a numpy matrix. I've called them tensors cause it's familiar to the AI engineers who use TensorFlow."
      ],
      "metadata": {
        "id": "5hxnQOZbagTx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Conv2DLayer:\n",
        "  '''\n",
        "    This class initializes the convolutional layer, the most basic and important layer in our CNN.\n",
        "  '''\n",
        "  def __init__(self, input_shape, filter_shape, stride, padding):\n",
        "    '''\n",
        "      This function initializes the convolutional layer.\n",
        "      Inputs:\n",
        "        - input_shape: The shape of the input structure (depth, height, width)\n",
        "        - filter_shape: The shape of the filter (num_filters, depth, height, width)\n",
        "        - stride: The number of steps the filter takes at each iteration\n",
        "        - padding: The amount of zero padding to be added to the input structure\n",
        "    '''\n",
        "    self.input_shape = input_shape\n",
        "    self.input_depth = input_shape[0]\n",
        "\n",
        "    self.num_filters = filter_shape[0]\n",
        "    self.filter_size = filter_shape[2]\n",
        "    self.stride = stride\n",
        "    self.padding = padding\n",
        "\n",
        "    #self.output_shape = (depth, input_height - filter_size + 1, input_width - filter_size + 1)\n",
        "    fan_in = input_shape[0] * filter_shape[2] * filter_shape[2]\n",
        "    self.filter_shape = filter_shape\n",
        "    self.filters = np.random.randn(filter_shape[0], input_shape[0], filter_shape[2], filter_shape[2])/np.sqrt(fan_in/2)\n",
        "    self.biases = np.zeros(filter_shape[0])\n",
        "\n",
        "  def zero_pad(self, input):\n",
        "    '''\n",
        "    This function pads the input with zeros to a certain degree.\n",
        "    Input:\n",
        "      - input: 2D array (H_1 x W_1)\n",
        "    Output:\n",
        "      - padded_input: 2D  padded array (H_1+padding X W_2+padding)\n",
        "    '''\n",
        "\n",
        "    D_1, H_1, W_1 = input.shape\n",
        "    #create a padded array of zeros\n",
        "    padded_input = np.zeros((D_1, H_1 + 2 * self.padding, W_1 + 2 * self.padding))\n",
        "    #copy the input structure into the centre of the padded array\n",
        "    padded_input[:, self.padding:self.padding + H_1, self.padding:self.padding + W_1] = input\n",
        "\n",
        "    return padded_input\n",
        "\n",
        "  def stride(self, input, h, w, filter_size):\n",
        "    '''\n",
        "    This function is meant to slide the filter along the structure volume a certain number of steps a\n",
        "    at each iteration.\n",
        "    Inputs:\n",
        "      - h: The height of the structure\n",
        "      - w: The width of the structure\n",
        "      - filter_size: The width and/or the height of the filter\n",
        "    Output:\n",
        "      - The input slice at a given iteration\n",
        "    '''\n",
        "    stride = self. stride\n",
        "     # Calculate the top-left corner of the current window\n",
        "    h_start = h * stride\n",
        "    w_start = w * stride\n",
        "\n",
        "    F_h, F_w = filter_size\n",
        "    # Extract the input slice\n",
        "    input_slice = input[:, h_start:h_start + F_h, w_start:w_start + F_w]\n",
        "\n",
        "  def forward_pass(self):\n",
        "    '''\n",
        "    This function does the actual convolution process that we described ealier in this cookbook.\n",
        "    '''\n",
        "    input = self.input_shape\n",
        "    stride = self.stride\n",
        "    padding = self.padding\n",
        "    filter_size = self.filter_size\n",
        "    filters = self.filters\n",
        "    biases = self.biases\n",
        "\n",
        "    (C_in, H_in, W_in) = self.input_shape\n",
        "    (num_filters, C_in_filter, F_h, F_w) = self.filter_shape\n",
        "\n",
        "    H_out = (H_in - F_h + 2*padding) // stride + 1\n",
        "    W_out = (W_in - F_w + 2*padding) // stride + 1\n",
        "\n",
        "    # Initialize the output tensor\n",
        "    output = np.zeros((num_filters, H_out, W_out))\n",
        "\n",
        "    padded_input = self.zero_pad(input)\n",
        "\n",
        "    # Perform convolution\n",
        "    for n in range(num_filters):  # Loop over each filter\n",
        "        for h in range(0, H_out):  # Loop over output height\n",
        "            for w in range(0, W_out):  # Loop over output width\n",
        "                input_slice = stride(padded_input, h, w, filter_size)\n",
        "\n",
        "                # Perform dot product\n",
        "                output[n, h, w] = np.sum(input_slice * filters[n]) + biases[n]\n",
        "\n",
        "    return output\n",
        "\n",
        "  def backward_pass(self, output_gradient, optimizer):\n",
        "    '''\n",
        "    This function performs the backward pass to the network that will\n",
        "    '''\n",
        "    filters_gradient= np.zeros(self.filter_shape)\n",
        "    input_gradient = np.zeros(self.input_shape)\n",
        "    bias_gradient = np.sum(output_gradient, axis=(1, 2)).reshape(self.biases.shape)\n",
        "\n",
        "    #compute the gradients\n",
        "    for i in range (self.num_filters):\n",
        "      for j in range (self. input_shape[0]):\n",
        "        filters_gradient[i, j] = signal.correlate2d(self.input[j], output_gradient[i], \"valid\")\n",
        "        input_gradient[j] += signal.convolve2d(output_gradient[i], self.kernels[i, j], \"full\")\n",
        "\n",
        "    self.filters, self.biases = optimizer.update(\n",
        "        self.filters, self.biases, filters_gradient, bias_gradient\n",
        "    )\n",
        "\n",
        "    return filters_gradient, input_gradient, bias_gradient"
      ],
      "metadata": {
        "id": "accTPHQFzfLv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Test the conv layer\n",
        "\n",
        "We need to know and see if what we're doing actually makes sense and works. To do that, we need to be sure that our layer is creating and initilazing the filters.\n",
        "\n",
        "To do this, we assume:\n",
        "1. An input of shape (1,32,32)\n",
        "2. We want to learn 32 features so assume 32 filters\n",
        "3. Filter matrix of shape (3,3)\n",
        "\n",
        "The other parameters don't really matter for us right now."
      ],
      "metadata": {
        "id": "iSBdCmt1wYBR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#testing the Conv2DLayer class to see if our layer is actually working\n",
        "input_shape = (1, 32, 32)\n",
        "conv_layer = Conv2DLayer(num_filters=32, filter_size=3, stride=1, padding=1, input_shape=input_shape, depth=1)\n",
        "\n",
        "print(f\"Filters shape: {conv_layer.filter_shape}\")\n",
        "print(f\"Filters: {conv_layer.filters}\")\n",
        "print(f\"Biases: {conv_layer.biases.shape}\")\n",
        "print(f\"Biases: {conv_layer.biases}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7S41fnrqwZKn",
        "outputId": "79471248-c9a1-4e5d-efae-7354de818204"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Filters shape: (1, 1, 3, 3)\n",
            "Filters: [[[[-0.77086145 -1.12572475 -0.23769746]\n",
            "   [ 0.30934366 -0.06627308 -0.23660901]\n",
            "   [ 0.35943154 -0.11117635  0.10582584]]]\n",
            "\n",
            "\n",
            " [[[ 0.38661059 -0.16423421  0.10269236]\n",
            "   [ 0.64298742 -0.07480266 -0.4915935 ]\n",
            "   [-0.54112349  0.57341941 -0.33600652]]]\n",
            "\n",
            "\n",
            " [[[ 0.09249641  0.14359097  0.10823315]\n",
            "   [-0.11697796 -0.57625676 -0.11557431]\n",
            "   [-0.18479033  0.21788764 -0.76555109]]]\n",
            "\n",
            "\n",
            " [[[-0.57658894 -0.61601796 -0.18299021]\n",
            "   [-0.02229058  0.52362172  0.04523191]\n",
            "   [ 0.35868255  0.26983439  0.62550827]]]\n",
            "\n",
            "\n",
            " [[[ 0.53095718  0.15639279  0.18376651]\n",
            "   [ 0.11092405 -0.07730636 -0.43651223]\n",
            "   [-0.17792025  0.66230161 -0.5030989 ]]]\n",
            "\n",
            "\n",
            " [[[ 0.48227481 -0.79866884  0.78630629]\n",
            "   [-0.31932405 -0.4224309  -0.77922378]\n",
            "   [ 0.49383822 -0.46682396 -0.2093662 ]]]\n",
            "\n",
            "\n",
            " [[[-0.72718066 -0.28478991 -0.2664904 ]\n",
            "   [ 0.27084903  0.07652734 -0.04773216]\n",
            "   [-0.62402334  0.05720462  0.09731497]]]\n",
            "\n",
            "\n",
            " [[[ 0.68417628 -0.50793259 -0.28599992]\n",
            "   [ 0.19186677 -0.61705473 -0.04251947]\n",
            "   [-0.04304427  0.64117204  0.45633593]]]\n",
            "\n",
            "\n",
            " [[[ 0.07486703 -0.52567003  0.55318842]\n",
            "   [-0.45203391 -0.42040704  0.63413748]\n",
            "   [-0.11430737 -0.2551412   0.48847718]]]\n",
            "\n",
            "\n",
            " [[[ 1.05407821  0.41794127 -0.02240064]\n",
            "   [-0.23136338  0.81419653  0.35566389]\n",
            "   [-0.95100927  0.26228955 -0.14028173]]]\n",
            "\n",
            "\n",
            " [[[-0.38343117 -0.02624685  0.04435814]\n",
            "   [ 0.44592591 -0.415212   -0.22959864]\n",
            "   [-0.23983991 -0.19512197 -0.39649814]]]\n",
            "\n",
            "\n",
            " [[[ 0.27774194 -0.84253583  0.58505783]\n",
            "   [ 0.17480938 -0.36923435  0.80134534]\n",
            "   [-0.96818541 -0.05886787 -0.6710401 ]]]\n",
            "\n",
            "\n",
            " [[[ 0.17158997 -0.12502108 -0.0804893 ]\n",
            "   [ 0.73758981 -0.04108223  0.35283827]\n",
            "   [-0.3486544  -0.58079325 -0.24831927]]]\n",
            "\n",
            "\n",
            " [[[ 0.00478523 -0.34939102  0.24388333]\n",
            "   [-0.01067354 -0.84350489 -0.57215315]\n",
            "   [-1.03151018 -0.73559863  0.46232053]]]\n",
            "\n",
            "\n",
            " [[[-0.95804039  0.22396667 -0.07615312]\n",
            "   [-0.79782446  0.15096766  1.01568563]\n",
            "   [ 0.68639772  0.56856732 -0.37251858]]]\n",
            "\n",
            "\n",
            " [[[ 0.24454742  0.1307915  -0.10279797]\n",
            "   [ 0.43801647  0.06248233 -0.11775252]\n",
            "   [-0.10067709  0.02340954 -0.06790364]]]\n",
            "\n",
            "\n",
            " [[[ 0.36886003  0.44634427 -0.0652093 ]\n",
            "   [ 0.77374755 -0.80339277  0.02480671]\n",
            "   [-0.36391363  1.01804414 -0.15330112]]]\n",
            "\n",
            "\n",
            " [[[ 0.37854561 -0.19917979 -0.95365192]\n",
            "   [ 0.01261324  0.25096028  0.03108482]\n",
            "   [-0.25434123 -0.47928837 -0.59262899]]]\n",
            "\n",
            "\n",
            " [[[ 0.42166693  0.31814164  0.25613322]\n",
            "   [-0.00243618 -0.34667329  1.22645569]\n",
            "   [ 0.34998585  0.79406122 -0.00815771]]]\n",
            "\n",
            "\n",
            " [[[ 0.70489283  0.30858713  0.55379418]\n",
            "   [ 0.17912716  1.02616851 -0.2593737 ]\n",
            "   [-1.14029062  0.63604358  0.39887481]]]\n",
            "\n",
            "\n",
            " [[[ 0.47559636  0.76312783 -0.06782678]\n",
            "   [ 0.3104234   0.71480988  0.47352216]\n",
            "   [ 0.01773129  0.10966866  0.22907593]]]\n",
            "\n",
            "\n",
            " [[[ 0.27470213  1.10813937  0.12654277]\n",
            "   [-0.6710412  -0.441058   -0.29763502]\n",
            "   [-0.8730375  -0.28029437  1.53005109]]]\n",
            "\n",
            "\n",
            " [[[-1.22470178  0.33792331 -0.23515468]\n",
            "   [-0.32929793  0.36182525 -0.24831659]\n",
            "   [-0.65742624  0.62932916  0.20921573]]]\n",
            "\n",
            "\n",
            " [[[-0.34646806  0.17829411  0.50072772]\n",
            "   [ 0.29396694 -0.80978001  0.50388189]\n",
            "   [-1.29043841 -0.0153633  -0.16781365]]]\n",
            "\n",
            "\n",
            " [[[-0.56818157  0.04339916 -0.72467909]\n",
            "   [-0.56046723 -0.37020211 -0.86751719]\n",
            "   [ 0.18768978 -0.68573899 -0.61765977]]]\n",
            "\n",
            "\n",
            " [[[ 0.03588971  0.64767102 -0.41324241]\n",
            "   [ 0.36893134 -0.2656304  -0.22837829]\n",
            "   [ 0.19544972 -0.28527184  0.08262038]]]\n",
            "\n",
            "\n",
            " [[[ 0.27211675 -0.38773907  0.10925792]\n",
            "   [ 0.33427575  0.49558536  0.13486883]\n",
            "   [-0.00366812  0.87310026  0.44954195]]]\n",
            "\n",
            "\n",
            " [[[-0.07284406 -0.53996579  0.43542035]\n",
            "   [-0.27479302  0.09363707 -0.96698418]\n",
            "   [-0.4153271   0.64368987 -0.4614983 ]]]\n",
            "\n",
            "\n",
            " [[[-0.20856605  0.31436242 -0.16743741]\n",
            "   [ 0.15747718 -0.9360222   0.47919053]\n",
            "   [-0.82972199 -0.32611751  0.66597314]]]\n",
            "\n",
            "\n",
            " [[[-0.5414256  -0.80142274  0.30105898]\n",
            "   [-0.07789152  0.17812994 -0.12247877]\n",
            "   [-0.21489515  0.69463997 -0.4598535 ]]]\n",
            "\n",
            "\n",
            " [[[ 0.79969576 -0.64404157 -0.95292961]\n",
            "   [ 0.21361501 -0.02784317  0.14346921]\n",
            "   [-0.48633959  0.32413553 -0.30189335]]]\n",
            "\n",
            "\n",
            " [[[-1.37683886  0.33024849 -0.1148674 ]\n",
            "   [ 0.67862597 -0.57091428  0.88791664]\n",
            "   [-0.51239468 -0.42444505  0.56086295]]]]\n",
            "Biases: (32,)\n",
            "Biases: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Weights and biases are being initialized correctly with the correct shape"
      ],
      "metadata": {
        "id": "mStdj03axHTH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Optimization**"
      ],
      "metadata": {
        "id": "kgt9cTobgA1o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now at this point of our existence, we have fully created and written our convolutional layer. Everything in that layer has implemented acccording to our theory that we've been cooking up in the markdowns. We've done all the thing that are needed for conv layer: padding, setting our stride, forward pass and backward pass. At the end of the backpass, we have our gradients.\n",
        "\n",
        "What do we do with these gradients now? Well, so as to learn, the network has to update the weights to reduce the error loss and it has to do this iteratively. The network does this through something called optimization.\n",
        "\n",
        "If you want to imagine how this works, think of it as walking down a valley and you want to get to the minimum point. Imagine our loss function giving us some curvature where the global minima is a point where $J(Θ) ≈ 0$ such that $J$ is our loss function. With our weights, we would like to take small steps in the steepest descent/ascent to get to the minima.\n",
        "\n",
        "To do this updates/ take these steps, we use an optimization algorithm. There are so many algorithms to do this, such as:\n",
        "1. Stochastic Gradient Descent (SGD)\n",
        "2. SGD + Momentum\n",
        "3. AdaGrad\n",
        "4. DeGrad\n",
        "5. RSMprop\n",
        "6. Adaptive Movement eStimation (Adams)\n",
        "\n",
        "In this project, we'll be implementing SGD with momentum. Why? Initially, I had planned on doing Adams and quickly realised how much explaining that would require. SGD is fairly simple. A small adjustment would give us SGD with momentum. Also our network is very shallow. So, let's get to it."
      ],
      "metadata": {
        "id": "S3jEu58EgUXn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Stochastic Gradient Descent (SGD)\n",
        "\n",
        "We'll start here and build up to adding some momentum and what that even means. Like I had said earlier, to optimize an algorithm/model/NN, we walk down the path of steepest descent/ascent till we get to some global minima. This is called 'gradient descent'\n",
        "> You may see some across the term 'the critical point' denoted with a $Θ^*$. Be aware that this doesn't mean the global minima. It is a candidate point that can be considered for both a local and global minima but has not been proven to be either.\n",
        "\n",
        "SGD **is** gradient descent with a small alteration. Instead of taking the whole dataset to learn, it uses just a small subset of it. It's the most basic algorithm but has some problems:\n",
        "1. What if the loss changes quickly in some dimension and slow in other dimensions?\n",
        "- Well, SGD will end up progressing very fast in some directions and very slowly in some directions leading to a very zig-zag, jittery pattern that is very bad for learning.\n",
        "> When this occurs, the loss function is said to a high condition. This means that the ratio of the largest to the smallest eigenvalue of the Hesian matrix is large. The best way to think of this is using a taco shell, where one side is steep (high curvature), and the other remains flat (low curvature); (i.e anisotropic).\n",
        ">> Just a quick btw. A hesian matrix is a sqaure matrix of second-order partial derivatives of a scala-valued function that describes the curvature of a function with many variables.\n",
        "\n",
        "2.  What if the loss a local minima or saddle point?\n",
        "- Well, the zero gradient at this point will make SGD get stuck and assuming it's gotten to the global minima.\n",
        "3. SGD takes random minibatches of data from the test data. This can lead to noise.\n",
        "\n",
        "We've talked about SGD's problems but what even does our SGD look like, mathematically. Well, the formula for it is:\n",
        "$$W_{t+1} := W_t - α⋅∇f(W_t)$$\n",
        "where:\n",
        "- $W_{t+1}$ is the new weight\n",
        "- $W_t$ is the value of the current weight\n",
        "- $α$ is the learning rate\n",
        "- $∇$ is the result of the gradient computation\n",
        "- $f(W_t)$ is our heuristic function\n",
        "\n",
        "When I said that this is the most basic and simple optimization algorithm, I wasn't joking. It really is. The equation above is a simple update rule that allows us to update our weights (learn). But this is prome to all the issues I mentioned earlier.\n"
      ],
      "metadata": {
        "id": "2mM2SC_jrHzB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### SGD + Momentum\n",
        "To solve the issues I mentioned above, we add momentum to our weight to build up \"velocity\" as a running mean. How does this help? Well, this way anytime we take a step we also have this momentum. The best analogy to use here is think of a ball that is rolling down a hill. This ball will remain in motion until it reaches somewhere flat. The only thing that would keep it moving is momentum till it gets to another descent where it will roll even more. That's the intution behind it.\n",
        "\n",
        "The real reasoning: as the point moves down towards a critical point, it builds up a momentum which is the average of the gradients as it proceeds. This would help it combat the issue of zero-gradients at critical points as steps will be taken using the velocity and the gradient at that point. There are 2 types of Momentum:\n",
        "1. Standard momentum: We take a step using the gradient and then add the velocity from that point\n",
        "2. Nestrov momentum: We take a step using the velocity, find the gradient at the new point, add them, then take a step from the original.\n",
        "\n",
        "For this project, we'll be implementing Nestrov's momentum, since it's always the better choice, in my opinion. But first, let's see the formulation for standard momentum and build off from it.\n",
        "\n",
        "Standard momentum:\n",
        "> $$V_{t+1} = ρV_t + ∇f(X_t) $$\n",
        "$$X_{t+1} = X_t - αV_{t+1}$$\n",
        "> where:\n",
        "- $ρ$ is the momentum coefficient that controls the influence of past gradients (usually a high number like $0.9$ or $0.99$)\n",
        "- $α$ is obviously the learning rate\n",
        "- $∇f(x_t)$ is the gradient from the current point\n",
        "- $V_t$ is the velocity at a certain point\n",
        ">> To get some intuition about what is going on here: the first formula (velocity formula) computes the 'lookahead' position, where the gradient will end up after a step. The second part of the formula is the update rule that uses the velocity to update the weight.\n",
        "\n",
        "From this, we can build up to Nestrov's momentum. The only change we'll make to the formula above is that in Nestrov's momentum, we take a step first using the velocity and then calculate the gradient from there. The forumla:\n",
        "> $$V_{t+1} = ρV_t - α∇f(x_t + ρV_t)$$\n",
        "$$X_{t+1} = X_t + V_{t+1}$$\n",
        "The forumla is correct, but we often want to update terms in terms of $x_t$, $∇f(x_t)$. So we can rewrite our formula a bit so it's easier to implement in code:\n",
        "$$V_{t+1} = ρV_t - α∇f({x^̃ }_t)$$\n",
        "$${x^̃ }_{t+1} = \\quad {x^̃ }_t - ρv_t + (1+ρ)v_{t+1} \\quad= {x^̃ } + v_{t+1} + \\rho(v_{t+1} - v_t)$$\n",
        "where:\n",
        "- ${x^̃}_t$ is the 'lookahead'; the estimated point where gradients will be calculated\n",
        "- Ealier definitions of $\\rho$, $α$ and $V_t$ are retained\n",
        ">> The second formulation is a much easier thing to implement compared to the first one. In some texts, you may find it called 'Nestrov's Accelerated Momentum' but it's more or less the same thing but with more refinement. The refinement is brought by addding that $ρ(V_{t+1} - V_t)$ which provides a higher-order adjustment. Why?\n",
        ">> - Smoothening oscillations hence reducing overshooting\n",
        ">> - Adapting to change in direction of the gradient\n",
        ">> - Accleration in flat regions\n",
        "\n",
        "\n",
        "Let's see in code."
      ],
      "metadata": {
        "id": "ch4cNhTOxPml"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class SGD_NAG:\n",
        "    def __init__(self, learning_rate, momentum):\n",
        "        '''\n",
        "        Always have an __init__ function in your class, seriously, do it.\n",
        "        Inputs:\n",
        "          learning_rate: The step size for the optimization.\n",
        "          momentum: The momentum coefficient.\n",
        "        '''\n",
        "\n",
        "        self.learning_rate = learning_rate\n",
        "        self.momentum = momentum\n",
        "        self.velocity_filters = None\n",
        "        self.velocity_biases = None\n",
        "\n",
        "    def update(self, fitlers, biases, filters_grads, biases_grads):\n",
        "        '''\n",
        "        After calculating the gradients in the backpass, this function\n",
        "        will update the parameters using Nesterov Accelerated Gradient.\n",
        "        Inputs:\n",
        "          filters: The current filters for the convLayer\n",
        "          biases: The biases for the convLayer\n",
        "          filters_grads: The gradients of the filters\n",
        "          biases_grads: The gradients of the biases\n",
        "        Output:\n",
        "          Updated filters and biases.\n",
        "        '''\n",
        "\n",
        "        #Initalize the filter and biases velocity\n",
        "        if self.velocity_filters is None:\n",
        "            self.velocity_filters = np.zeros_like(self.filters)\n",
        "        if self. velocity_biases is None:\n",
        "            self.velocity_biases = np.zeros_like(self.biases)\n",
        "\n",
        "        #lookahead for the filters and biases\n",
        "        lookahead_filters = self.filters - self.momentum * self.velocity_filters\n",
        "        lookahead_biases = self.biases - self.momentum * self.velocity_biases\n",
        "\n",
        "        #update velocities\n",
        "        self.velocity_filters = self.momentum * self.velocity_filters - self.learning_rate * filters_grads\n",
        "        self.velocity_biases = self.momentum * self.velocity_biases - self.learning_rate * biases_grads\n",
        "\n",
        "        #update parameters\n",
        "        updated_filters = lookahead_filters + self.velocity_filters + self.momentum * (self.velocity_filters - self.momentum * self.velocity_filters)\n",
        "        updated_biases = lookahead_biases + self.velocity_biases + self.momentum * (self.velocity_biases - self.momentum * self.velocity_biases)\n",
        "\n",
        "        return updated_filters, updated_biases"
      ],
      "metadata": {
        "id": "LlkO9D-vuZk5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Activation Function**"
      ],
      "metadata": {
        "id": "-EO3SJhy-Lao"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This might honestly end up being the easiest thing we've implemented all throughout this cookbook. After each convolotional layer, we need to add something called an fucntion function. What is that? It's just a mathematical function that tells neurons how to respond to an input. And why do we need it? A few reasons actually:\n",
        "- The most important reason may be to introduce non-linerity in your network. This is the most important reason because non-linearity is needed to learn complex patterns in data (such as is the norm in the real-world).\n",
        "- They transform the input signal into an output that can be passed to whatever layer comes next\n",
        "- They choose which neurons to activate in the network and which ones to leave 'dead'.\n",
        "\n",
        "They are so many activation functions out there like:\n",
        "1. Sigmoid\n",
        "2. Tanh\n",
        "3. Rectified Linear Unit (ReLU)\n",
        "4. Leaky ReLU\n",
        "5. Paremetric ReLU (PReLU)\n",
        "6. Exponential Linear Unit (ELU)\n",
        "7. Maxout\n",
        "\n",
        "In this project, we will be using PReLU, but for explanation, we will start from ReLU."
      ],
      "metadata": {
        "id": "ANWa7P1A-Q3g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Rectified Linear Unit\n",
        "Ever heard of the phrase, \"beauty in simplicity\". Or maybe Occam's razor that states that among competing hypothesis, the simplest is the best. Or maybe the golden rule of computer science, K.I.S.S (Keep It Simple Stupid). ReLU is the embodiment of all these. It is the simplest activation function that you could implement and the most efficient computationally since it only does ONE simple mathematical operation: finding the maximum of 2 inputs. Just that. In fact the formula for it is just:\n",
        ">$$f(x) = max(0,x)$$\n",
        "> In code:\n",
        "> ```\n",
        "return np.max(0, x)\n",
        ">```\n",
        "\n",
        "Compared to the predeseccors of ReLU (Sigmoid and Tanh), this activation thrives in computational cost. It also solves a major problem that they had; saturation of neurons in the positive region. This means that the neuron's output are close to the asymptotic ends of a bounded activation function. I will not getinto all this in this notebook cause I'll have to explain other activations for that.\n",
        "\n",
        "As much as it does solve the saturation problem, it does have 2 unique problems of it's own:\n",
        "1. Not zero-centred. (Can be solved by sero-centering your data in the preprocessing step)\n",
        "2. Kills the gradient in half the graph. Since ReLU compares an input value (logit) with 0, it means any negative input value will be reduced to 0. This reduction will lead to the killing of some neurons.\n",
        "\n",
        "Often, this is ok, but to get just a little bit more efficieny from our NN, we have an activation function that attempts to solve the 2nd problem called Leaky ReLU."
      ],
      "metadata": {
        "id": "_D4u2BvhC1R_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Leaky ReLU\n",
        "Leaky ReLU is just an improvement of ReLU with only one change. Instead of comparing the inputs with $0$ we compare it with itself scaled by some very small positive value like $0.01$.\n",
        "> $$f(x) = max(0.01, x)$$ In code\n",
        ">```\n",
        "return np.max((0.01*x), x)\n",
        ">```\n",
        "\n",
        "Quite simple, really. A very small change that solves a relatively huge problem. With this implementation, the negative side does not die, but rather have a small, but significant ouptut.\n",
        "\n",
        "But we can take it a step further. The $0.01$ is just a constant in this case. However, we don't know if that's the best value. What if we could learn it like we do with all other paramters to find the optimal scalar value. Well, that gives us our activation function PReLU."
      ],
      "metadata": {
        "id": "4ngFRmPqH7uP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Parametric ReLU\n",
        "See why I had to start from ReLU to get here? Like I said, all we need to do is change something small to get a huge change. In this case, we get a learnable activation function.\n",
        "> $$f(x) = max(αx,x)$$\n",
        "where:\n",
        "- α is a learnable parameter that can't be learned using backprop\n",
        "\n",
        "To make a bit easier to implement in code, let's rewrite it a bit. (Be aware that all other forms of ReLU could be rewritten as follows, but we're only doing it with this one cause it's what we'll be implementing and makes backprop easier).\n",
        "> $$PReLU(x) = f(x) =\n",
        "\\left\\{\n",
        "\\begin{array}{11}\n",
        "x, & \\text{if }\\quad x > 0 \\\\\n",
        "α⋅x, & \\text{otherwise}\n",
        "\\end{array}\n",
        "\\right\\}$$\n",
        "> The forward pass in code:\n",
        ">```\n",
        "return np.where(x > 0, x, self.alpha * x)\n",
        ">```\n",
        "> The back pass in code:\n",
        "```\n",
        "dx = np.where(self.input > 0, dy, self.alpha*dy )\n",
        "self.alpha_grad = np.sum(dy * self.input * (self.input <= 0))\n",
        "```\n",
        "And that's it, honestly. Like I said, it's the simplest thing so far and we can use it for both the conv2D layer and FC layer.\n"
      ],
      "metadata": {
        "id": "2zvRDDBJJund"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Actual PReLU implementation"
      ],
      "metadata": {
        "id": "IrxnjCL2QfxF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PReLU():\n",
        "  def __init__(self, alpha=0.01):\n",
        "    '''\n",
        "      This function initializes the PReLU activation function. Seriously, initialize this stuff, makes life so much easier.\n",
        "    '''\n",
        "    self.alpha = alpha\n",
        "    self.alpha_grad = None\n",
        "\n",
        "  def forward(self, prev_layer_input):\n",
        "    '''\n",
        "      This function does the forward pass of the PReLU activation function.\n",
        "      Input:\n",
        "        prev_layer_input: The input from the previous layer, be it the conv2D or the FC layer.\n",
        "      Output:\n",
        "        The output of the PReLU activation function.\n",
        "    '''\n",
        "    #storing the input with self to use in the backprop fun\n",
        "    self.input = prev_layer_input\n",
        "\n",
        "    return np.where(prev_layer_input > 0, prev_layer_input, self.alpha * prev_layer_input)\n",
        "\n",
        "  def backward(self, dy):\n",
        "    '''\n",
        "      This function does the backward pass of the PReLU activation function.\n",
        "      Input:\n",
        "        dy: The gradient of the loss function with respect to the output of the PReLU activation function.\n",
        "      Output:\n",
        "        dx: The gradient of the loss function with respect to the input of the PReLU activation function.\n",
        "    '''\n",
        "    # Gradient of the activation with respect to the input\n",
        "    dx = np.where(self.input > 0, dy, self.alpha * dy)\n",
        "\n",
        "    # Gradient of alpha: sum of the gradients where input <= 0\n",
        "    self.alpha_grad = np.sum(dy * self.input * (self.input <= 0))\n",
        "\n",
        "    return dx"
      ],
      "metadata": {
        "id": "cgAMHAJYIgop"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Pooling Layer**"
      ],
      "metadata": {
        "id": "OrIRc28jYXhY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To the second simplest concept in this whole thing. Pooling is an operation whose sole purpose is to reduce the dimensionality of an input so as to avoid the curse of dimensionality. Basically, the pooling layer makes the network invariant to the small translations in the input.\n",
        "> The **curse of dimensionality** is a problem that arose from standard ML where the dimensions of the input are so large that it becomes computationally expensive to handle all the features. In standard ML, features are reduced using methods such as Principal Component Analysis (PCA).\n",
        "\n",
        "In computer vision, we don't use PCA to reduce dimensionality since our features are not vectors of integers. They are, in fact, matrices. We would like to retain the spatial structure of the input features (filters) while reducing the spatial dimensionality. Enter pooling. Pooling works by simply sliding a filter (often a $2$x$2$ or $3$x$3$ matrix) with no values across a feature map (at a given stride) and identifying the most important/dominant feature weights in the covered region.\n",
        "\n",
        "There are 2 major types of pooling:\n",
        "1. Max Pooling: In this type of pooling, the filter picks the largest feature in a given covered region.\n",
        "2. Average Pooling: For this, the filter does the average of the weights in the covered region and returns it as it's output.\n",
        "\n",
        "For this project, we'll be using max pooling since it's the most common type of pooling used in CNNs all over. Before that, this layer has only one formula that is used to calculate the output structure size. It's really simple and familiar to the formula we saw with the Conv2D layer for calculating the output size.\n",
        "\n",
        "Some notations:\n",
        "- $H_{in}$ is the input height.\n",
        "- $W_{in}$ is the input width.\n",
        "- $H_{out}$ is the output height.\n",
        "- $W_{out}$ is the output width.\n",
        "- $K_h$ is the height of the pooling window (filter size).\n",
        "- $K_w$ is the width of the pooling window.\n",
        "- $S_v$ is the vertical stride (how much the window moves down).\n",
        "- $S_h$ is the horizontal stride (how much the window moves right).\n",
        "- $P_v$ is the vertical padding (if any).\n",
        "- $P_h$ is the horizontal padding (if any).\n",
        "\n",
        ">$$H_{out} = \\left(\\frac{H_n - K_h + 2P_v}{S_v}\\right) +1$$\n",
        ">$$W_{out} = \\left(\\frac{W_n - K_w + 2P_h}{S_h}\\right) +1$$\n",
        "\n",
        "With this, we can confirm whether we're getting the right dimensions for the output\n",
        "\n",
        ">**NOTE:** The depth of the structure is ALWAYS retained. That should be obvious by now.\n",
        "\n",
        "The backward pass for the pooling layer works in the exact opposite manner as the forward pass. In the backpass, we 'rebuild' our feature map with only the max element (from the previous layer) and 0's in place of other elements that we don't know off."
      ],
      "metadata": {
        "id": "ZtIH78r6YdKU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Actual Pooling Layer"
      ],
      "metadata": {
        "id": "C85_C8lqdb-r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MaxPool2D():\n",
        "  def __init__(self, pool_size, stride, padding):\n",
        "    self.pool_size = pool_size\n",
        "    self.stride = stride\n",
        "    self.padding = padding\n",
        "    self.input_shape = None\n",
        "    self.input_map = None\n",
        "\n",
        "  def forward(self, feature_map):\n",
        "    '''\n",
        "    This function is meant to act as the pooling layer after a Conv2D layer\n",
        "    Inputs:\n",
        "      feature_map: This is the output of the Conv2D layer\n",
        "      pool_size: This is the size of the pooling filter\n",
        "      stride: This the steps to be taken by the filter\n",
        "      padding: This is the amount of zero padding to be added\n",
        "    Output:\n",
        "      output: This is the input structure with reduced spatial dimanesions of the pooling layer\n",
        "    '''\n",
        "    #account for any padding that may be added\n",
        "\n",
        "    self.input_shape = feature_map.shape\n",
        "    self.input_map = feature_map\n",
        "\n",
        "    if self.padding > 0:\n",
        "          feature_map = np.pad(feature_map,\n",
        "                        ((0,0), (0,0), (self.padding, self.padding), (self.padding, self.padding)),\n",
        "                        mode='constant',\n",
        "                        constant_values=0)\n",
        "\n",
        "    #Get the shapes for the input and the pooling filter\n",
        "    batch_size, channels, H_in, W_in = feature_map.shape\n",
        "    pool_h, pool_w = self.pool_size\n",
        "\n",
        "    #calculate the output size\n",
        "    W_out = (W_in - pool_w) // self.stride + 1\n",
        "    H_out = (H_in - pool_h) // self.stride + 1\n",
        "\n",
        "    H_out = max(1, H_out)\n",
        "    W_out = max(1, W_out)\n",
        "\n",
        "    self.max_indices = np.zeros_like(feature_map)\n",
        "\n",
        "    #initialize the output\n",
        "    output_map = np.zeros((batch_size, channels, H_out, W_out))\n",
        "\n",
        "    #create the window and sliding\n",
        "    for b in range(batch_size):\n",
        "      for c in range(channels):\n",
        "        for i in range(H_out):\n",
        "          for j in range(W_out):\n",
        "            #slide the window\n",
        "            window = feature_map[b, c, i*self.stride : i*self.stride+pool_h,\n",
        "                                j*self.stride : j*self.stride+pool_w]\n",
        "            #perform max pooling\n",
        "            output_map[b, c, i, j] = np.max(window)\n",
        "            max_idx = np.unravel_index(np.argmax(window), window.shape)\n",
        "            self.max_indices[b, c, i+max_idx[0], j+max_idx[1]] = 1\n",
        "\n",
        "\n",
        "    return output_map\n",
        "\n",
        "  def backward(self, dL_dOutput):\n",
        "    '''\n",
        "    This function is meant to propagate the feature map with repect to the input.\n",
        "    Basically give us our gradient the same shape as the input\n",
        "    Input:\n",
        "      dL_dOutput: Gradients passed in the backprop\n",
        "    '''\n",
        "    dL_dInput = np.zeros_like(self.input_map)\n",
        "    batch_size, channels, out_height, out_width = dL_dOutput.shape\n",
        "\n",
        "    for b in range(batch_size):\n",
        "        for c in range(channels):\n",
        "            for i in range(out_height):\n",
        "                for j in range(out_width):\n",
        "                    # Find the corresponding window in the input\n",
        "                    window_start_i = i * self.stride\n",
        "                    window_start_j = j * self.stride\n",
        "                    window_end_i = window_start_i + self.pool_size[0]\n",
        "                    window_end_j = window_start_j + self.pool_size[0]\n",
        "\n",
        "                    # Only the position of the max value gets the gradient\n",
        "                    max_mask = self.max_indices[b, c, window_start_i:window_end_i, window_start_j:window_end_j]\n",
        "                    dL_dInput[b, c, window_start_i:window_end_i, window_start_j:window_end_j] += dL_dOutput[b, c, i, j] * max_mask\n",
        "\n",
        "    return dL_dInput"
      ],
      "metadata": {
        "id": "nB70qvU0dhfr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Beautiful code but does it work? Let's test and see\n",
        "\n",
        "#create a test matrix that will work as our feature map\n",
        "test_input = np.array([\n",
        "     [1, 3, 2, 4],\n",
        "    [5, 6, 7, 8],\n",
        "    [9, 10, 11, 12],\n",
        "    [13, 14, 15, 16]\n",
        "])\n",
        "\n",
        "# Apply pooling with 2x2 window, stride 1, and padding 1\n",
        "output = MaxPool2D(test_input, pool_size=(2, 2), stride=1, padding=0)\n",
        "\n",
        "print(\"Output after pooling:\")\n",
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-jM2kVgDh0kL",
        "outputId": "9f77baaf-e1bc-433a-8f6f-911f483b40eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output after pooling:\n",
            "[[ 6.  7.  8.]\n",
            " [10. 11. 12.]\n",
            " [14. 15. 16.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Flattening layer**"
      ],
      "metadata": {
        "id": "ioqvOLRLyYKS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Remember how I said that the activation function is the simplest thing to implement here. Well, I sort of lied. The activation function was the simplest thing mathematically (so was the pooling layer since they're both just finding the max), but flattening may be the simplest thing completely.\n",
        "\n",
        "So, what's the purpose of this layer? Well, after everything comes from the Conv2D layer and from the pooling layer, it has to go to a fully connected (dense) layer that can look at the features at a high level and do some reasoning. Now, the issue with this is that the FC layer expects a 1D vector as input. At this point, everything we've been doing gives a matrix as an ouput. We have to fix this, and we do this by 'flattening'. This layer has no math. It's all code, so let's get to it.\n",
        "\n",
        "Basicaly, what we're doing is changing a tensor from a shape of $(D,H,W)$ into a 1D shape $(D⋅H\\cdot W)$"
      ],
      "metadata": {
        "id": "SthdRYC4ygaS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Flatten:\n",
        "  def __init__(self):\n",
        "    '''\n",
        "    An initialization function for the flatten layer.\n",
        "    I think by now you should know what I'm going to say for this\n",
        "    '''\n",
        "    self.input_shape = None #to store the original shape for unflatening\n",
        "\n",
        "  def flatten(self, input_tensor):\n",
        "    '''\n",
        "    Reshape the input tensor into a 1D vector.\n",
        "    Input:\n",
        "      input_tensor: The input tensor to be flattened.\n",
        "    Output:\n",
        "      The flattened tensor.\n",
        "    '''\n",
        "    self.input_shape = input_tensor.shape\n",
        "    return input_tensor.reshape(-1)\n",
        "\n",
        "  def unflatten(self, output_gradient):\n",
        "    '''\n",
        "    Reshape the output gradient from the FC layer into original shape\n",
        "    Input:\n",
        "      output_gradient: The output gradient from the FC layer.\n",
        "    Output:\n",
        "      The unflattened gradient.\n",
        "    '''\n",
        "    return output_gradient.reshape(self.input_shape)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "EY-CArs8z6jp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "To test whether our flattening layer is actually working, we'll assume a 3x4x4 output\n",
        "from our pooling layer\n",
        "'''\n",
        "\n",
        "# Example output from a pooling layer\n",
        "pool_output = np.random.rand(3, 4, 4)\n",
        "print(pool_output)\n",
        "\n",
        "#initialize the flattening layer\n",
        "flatten_layer = Flatten()\n",
        "\n",
        "# flattening pass\n",
        "flattened_output = flatten_layer.flatten(pool_output)\n",
        "print(\"Flat output:\")\n",
        "print(flattened_output)\n",
        "print(f\"Flat output shape: flattened_output.shape\")  # Should print (48,)\n",
        "\n",
        "# unflattening Pass\n",
        "grad = np.random.rand(48)  # Example gradient from the dense layer\n",
        "reshaped_grad = flatten_layer.unflatten(grad)\n",
        "print(\"Reshaped grad:\")\n",
        "print(reshaped_grad)\n",
        "print(f\"Reshaped grad shape: {reshaped_grad.shape}\")  # Should print (3, 4, 4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ynKJ8yc0KBq",
        "outputId": "f2853153-0182-427f-cd90-4918879813f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[[0.21748052 0.43818825 0.21033297 0.88470435]\n",
            "  [0.39478603 0.89036283 0.76333468 0.18929517]\n",
            "  [0.4741817  0.7623871  0.87488044 0.78947741]\n",
            "  [0.18920341 0.11311195 0.96567126 0.35670834]]\n",
            "\n",
            " [[0.94550993 0.26021057 0.01891305 0.94974973]\n",
            "  [0.52850599 0.21689779 0.30043856 0.16167789]\n",
            "  [0.69240044 0.42257828 0.01353891 0.78531777]\n",
            "  [0.34458198 0.09003186 0.65676747 0.30411179]]\n",
            "\n",
            " [[0.20505284 0.62440074 0.41007311 0.6165329 ]\n",
            "  [0.95497529 0.69150772 0.12544577 0.04050557]\n",
            "  [0.66568    0.83925949 0.58459667 0.78541371]\n",
            "  [0.00942487 0.02456078 0.30717238 0.60611551]]]\n",
            "Flat output:\n",
            "[0.21748052 0.43818825 0.21033297 0.88470435 0.39478603 0.89036283\n",
            " 0.76333468 0.18929517 0.4741817  0.7623871  0.87488044 0.78947741\n",
            " 0.18920341 0.11311195 0.96567126 0.35670834 0.94550993 0.26021057\n",
            " 0.01891305 0.94974973 0.52850599 0.21689779 0.30043856 0.16167789\n",
            " 0.69240044 0.42257828 0.01353891 0.78531777 0.34458198 0.09003186\n",
            " 0.65676747 0.30411179 0.20505284 0.62440074 0.41007311 0.6165329\n",
            " 0.95497529 0.69150772 0.12544577 0.04050557 0.66568    0.83925949\n",
            " 0.58459667 0.78541371 0.00942487 0.02456078 0.30717238 0.60611551]\n",
            "Flat output shape: flattened_output.shape\n",
            "Reshaped grad:\n",
            "[[[0.40060697 0.69846509 0.16311384 0.44462588]\n",
            "  [0.86489998 0.43859941 0.77114382 0.25073126]\n",
            "  [0.52232469 0.29043402 0.08562772 0.51228374]\n",
            "  [0.68483388 0.47860812 0.64913419 0.67259433]]\n",
            "\n",
            " [[0.82046032 0.18428763 0.28987004 0.00729461]\n",
            "  [0.23056897 0.27371556 0.19978798 0.07899503]\n",
            "  [0.9770359  0.1733322  0.49252211 0.05889171]\n",
            "  [0.00579324 0.65739    0.24412615 0.81533371]]\n",
            "\n",
            " [[0.4105274  0.93040938 0.53440808 0.06279856]\n",
            "  [0.57374421 0.0861842  0.37897488 0.86538725]\n",
            "  [0.98559948 0.1595825  0.59438315 0.73100668]\n",
            "  [0.35920278 0.46325345 0.48753747 0.13510697]]]\n",
            "Reshaped grad shape: (3, 4, 4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Fully Connected Layer (Dense)**"
      ],
      "metadata": {
        "id": "f7RTLod44CG6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Seems we're back to some math, but this an easy concept. Before this, we discussed the flattening layer which takes in a 3-D tensor and 'flattens' it into a 1-D vector to serve as input to an FC layer. That's what we're about to talk about. So, first of all, what even is an FC layer? Think of it as the reasoning factor for your CNN. The Conv2D layer is meant to 'look' at the image and draw some features, but the FC layer is meant to use those extracted features to reason.\n",
        "\n",
        "In this layer, every single input is connected to every single neuron, which means each neuron from the previous layer is connected to every single neuron of the next layer, hence the name fully connected layer. This means that every single neuron sees all inputs and makes some decision on it. These decisions are used to come up with some probabilty to make a final decision (to be discussed next). Also, this layer brings linearity back to the input. We will, in this project, choose to introduce non-linearity using an activation function, though.\n"
      ],
      "metadata": {
        "id": "HsHapEox6r_C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Forward pass"
      ],
      "metadata": {
        "id": "2FXtPrq7A1HK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Mathematically, this is a very simple layer. It uses something from math that we all know about called 'the equation of a straight line'. Remember that?\n",
        ">$$y = m\\cdot x + c$$\n",
        "\n",
        "That simple equation from high-school will be the foundation of this layer. For this layer we have a bit of a different formulation:\n",
        ">$$z = W \\cdot x + b$$\n",
        "> where:\n",
        "- $z$ is the output from a given neuron\n",
        "- $W$ is the feature weight matrix of shape $m$ x $n$ where\n",
        "  - $m$ is the number of output neurons (neurons in the next layer)\n",
        "  - $n$ is the number of input neurons (input features)\n",
        "- $x$ is the input vector of shape $n$ x $1$\n",
        "- $b$ is the bias vector of shape $m$ x $1$\n",
        "\n",
        "We can also choose to be a bit more strict and write it in terms of it's elements. It does look a lot more complicated, but it really isn't. The notaion is retained:\n",
        ">$$z_i = \\sum_{j=1}^n W_{ij} \\cdot x_j + b_j$$\n",
        "\n",
        " If we are to implement an activation function, as we will, we'll have to add one more thing to our forward pass:\n",
        "> $$a = f(z)$$\n",
        "> where:\n",
        " - $a$ is the output from our activation function\n",
        " - $f(z)$ is our activation function\n",
        " - $z$ is the output from the linear function $W \\cdot x +b$\n",
        "\n",
        "In our case, we will be applying the PReLU that we wrote earlier.\n",
        "\n",
        "And that's it. That's the whole forward pass for our FC layer.\n"
      ],
      "metadata": {
        "id": "j9q4o6mHA3MD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Backward pass"
      ],
      "metadata": {
        "id": "Vexg75rkA83K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The backward pass for this layer is very, very similar to what we saw for the Conv2D layer. It's actually the exact same but much simpler since we're not dealing with matrices this time. For the backpass, we compute the gradient of the loss w.r.t the weights, biases, and inputs.\n",
        "\n",
        "We'll start assuming that we won't add an activation function since we need to build from there. The formulas for the computations are as follows: (Remember in this cookbook, I'm keeping things simple, and will not do any derivations. You are free to derive them if you want, though)\n",
        "1. Gradient w.r.t the weights:\n",
        "> $$\\frac{∂L}{∂W} = \\frac{∂L}{∂z} \\cdot x^T$$\n",
        ">where:\n",
        "  - $\\frac{∂L}{∂z}$ is the derivative (gradient) of the loss w.r.t the layer output; recieved from the next layer\n",
        "  - $x^T$ is the transposed input vector\n",
        "\n",
        "2. Gradient w.r.t the biases:\n",
        "> $$\\frac{∂L}{∂b} = \\frac{∂L}{∂z}$$\n",
        "> If done using batch-processing, we can sum over all the examples since the bias is shared among all the examples. Therefore:\n",
        "> $$\\frac{∂L}{∂b} = \\sum \\frac{∂L}{∂z}$$\n",
        "\n",
        "3. Gradient w.r.t the input:\n",
        ">$$\\frac{∂L}{∂x} = W^T \\cdot \\frac{∂z}{∂x}$$\n",
        "\n",
        "But what if we had an activation function. Well, if we do, then things change a bit cause we need to account for the derivative of the activation function. That means that the function we are focusing on now will be $a = f(z)$.\n",
        "\n",
        "1. Gradient w.r.t the weights:\n",
        "> $$\\frac{∂L}{∂W} = \\left(\\frac{∂L}{∂a} \\odot f'(z)\\right) \\cdot x^T$$\n",
        ">where:\n",
        "  - $\\frac{∂L}{∂a}$ is the derivative (gradient) of the loss w.r.t the activation function; recieved from the next layer\n",
        "  - $x^T$ is the transposed input vector\n",
        "\n",
        "2. Gradient w.r.t the biases:\n",
        "> $$\\frac{∂L}{∂b} = \\sum \\left(\\frac{∂L}{∂a} \\odot f'(z)\\right) $$\n",
        "\n",
        "3. Gradient w.r.t the input:\n",
        ">$$\\frac{∂L}{∂x} = W^T \\cdot \\left(\\frac{∂L}{∂a} \\odot f'(z)\\right)$$\n",
        "\n",
        "Note that the $⊙$ represent element-wise/pointwise multiplication.\n",
        "\n",
        "A lot to digest, so let's put it all in code."
      ],
      "metadata": {
        "id": "7r5JI08cA-4i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Actual FC layer"
      ],
      "metadata": {
        "id": "39a7o6vEd_L7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class FullyConnectedLayer:\n",
        "    def __init__(self, input_neurons_num, output_neurons_num, activation_function=None, activation_derivative=None):\n",
        "        '''\n",
        "        Initialize the fully connected layer. No comment!\n",
        "        Input:\n",
        "          input_neurons_num: Number of input neurons.\n",
        "          output_neurons_num: Number of output neurons.\n",
        "          activation_function: The activation function to apply.\n",
        "          activation_derivative: The derivative of the activation function.\n",
        "        '''\n",
        "        self.input_size = input_neurons_num\n",
        "        self.output_size = output_neurons_num\n",
        "        self.activation_function = activation_function\n",
        "        self.activation_derivative = activation_derivative\n",
        "\n",
        "        # Initialize weights and biases with He initialization\n",
        "        self.weights = np.random.randn(output_neurons_num, input_neurons_num) * np.sqrt(2 / input_neurons_num)\n",
        "        self.biases = np.zeros((output_neurons_num, 1))\n",
        "\n",
        "    def forward(self, input_data):\n",
        "        '''\n",
        "        Perform the forward pass through the FC layer.\n",
        "        Input:\n",
        "          input_data: Input data of shape (input_neurons_num, batch_size).\n",
        "        Output:\n",
        "          Output data after applying the activation function.\n",
        "        '''\n",
        "\n",
        "        self.input = input_data  # Store input for use in backward pass\n",
        "        self.z = np.dot(self.weights, input_data) + self.biases\n",
        "        if self.activation_function:\n",
        "            return self.activation_function(self.z)\n",
        "        else:\n",
        "            return self.z\n",
        "\n",
        "    def backward(self, output_gradient, learning_rate):\n",
        "        '''\n",
        "        Perform the backward pass through the FC layer.\n",
        "        Input:\n",
        "          output_gradient: Gradient of the loss w.r.t. the output (a) of this layer.\n",
        "          learning_rate: Learning rate for weight updates.\n",
        "        Output:\n",
        "          dx Gradient of the loss w.r.t. the input (x) of this layer.\n",
        "        '''\n",
        "\n",
        "        # Compute gradient w.r.t. z\n",
        "        if self.activation_derivative:\n",
        "            dz = output_gradient * self.activation_derivative(self.z)  # Element-wise multiplication\n",
        "        else:\n",
        "            dz = output_gradient\n",
        "\n",
        "        # Compute gradients\n",
        "        dw = np.dot(dz, self.input.T) / self.input.shape[1]  # Weight gradient\n",
        "        db = np.sum(dz, axis=1, keepdims=True) / self.input.shape[1]  # Bias gradient\n",
        "        dx = np.dot(self.weights.T, dz)  # Input gradient\n",
        "\n",
        "        # Update weights and biases\n",
        "        self.weights -= learning_rate * dw\n",
        "        self.biases -= learning_rate * db\n",
        "\n",
        "        return dx\n"
      ],
      "metadata": {
        "id": "jBtm74-pPMy8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#As usual, we need to test whether what we're doing makes sense or not\n",
        "# create some test data\n",
        "input_data = np.random.randn(128, 32)  # Input of shape (input_size, batch_size)\n",
        "output_gradient = np.random.randn(64, 32)  # Dummy gradient from next layer\n",
        "\n",
        "#initialize an instance of PReLU\n",
        "actiavtion = PReLU()\n",
        "\n",
        "#initialize a FC layer\n",
        "fc_layer = FullyConnectedLayer(128, 64, activation_function=actiavtion.forward, activation_derivative=actiavtion.backward)\n",
        "\n",
        "output = fc_layer.forward(input_data)\n",
        "print(output.shape)\n",
        "print(output)\n",
        "\n",
        "# Backward pass\n",
        "input_gradient = fc_layer.backward(output_gradient, learning_rate=0.01)\n",
        "print(input_gradient.shape)\n",
        "print(input_gradient)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gCCZCNYUa0K0",
        "outputId": "17e58bc2-6e97-437a-aa9b-fcbe5073c1e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 32)\n",
            "[[ 1.59732058e-01 -9.01600614e-03  2.29443749e+00 ... -2.81974620e-03\n",
            "  -2.04431051e-02 -1.72507398e-02]\n",
            " [-3.19568989e-02  1.51436586e+00 -8.38444095e-03 ... -3.42938710e-03\n",
            "  -4.88291027e-03  1.29221054e+00]\n",
            " [ 1.65197096e+00 -3.73681929e-04 -8.30110780e-03 ...  1.34533932e+00\n",
            "  -1.67639117e-02 -1.07803963e-02]\n",
            " ...\n",
            " [-3.94632455e-03 -1.15067556e-04 -4.31311053e-04 ...  7.43291669e-01\n",
            "  -7.19238499e-03 -1.92792588e-02]\n",
            " [ 3.21997497e-01  1.42102273e+00 -1.59259513e-02 ...  6.06669440e-01\n",
            "   7.06317642e-01 -1.67623809e-02]\n",
            " [ 1.62881343e+00 -1.39328038e-02  1.87068450e+00 ...  3.52945141e+00\n",
            "   1.90272467e+00  9.37999934e-01]]\n",
            "(128, 32)\n",
            "[[ 2.82525596e-02 -2.02839716e+00  7.76202472e-01 ...  1.83626004e+00\n",
            "   5.58037100e-01  1.05835937e+00]\n",
            " [ 8.41143364e-01 -2.03476270e+00  1.17982482e+00 ...  9.55932375e-01\n",
            "   4.89229950e-01 -6.80444188e-01]\n",
            " [ 4.47285094e-01  2.28066059e+00 -9.65314256e-01 ...  1.24617036e+00\n",
            "   1.17733022e+00  3.09543604e-02]\n",
            " ...\n",
            " [ 1.44632294e-01 -9.89406743e-01 -2.74248514e-01 ...  1.10342274e+00\n",
            "  -2.23990456e+00  2.25789614e-04]\n",
            " [-2.87827563e-01  4.33926162e-02 -2.43142795e+00 ...  1.18567796e+00\n",
            "   4.20999674e-02  9.65700540e-01]\n",
            " [ 9.87633403e-01 -1.73407285e+00 -2.13178494e+00 ... -5.41365056e-01\n",
            "  -1.64079362e+00  6.32508755e-01]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Softmax classifier**"
      ],
      "metadata": {
        "id": "vpomnUokjq5z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now after all that we've done that, we need to make decision and make some classifications. How do we do this? Well, if you're coming from basic ML, you already know we use probabilites for classification. But so far, we haven't dealt with probabilities, yet, but we have worked with non-linearity in the form of ReLU. Remember that was the function of the activation function in the first place. However, ReLU only does a max opeartion so we can't use it. We need another activation function at the last FC layer that will give us multiple probabilites for all our classes (10 classes for our MNIST dataset).\n",
        "\n",
        "The most common activation function for this is called softmax. It's very similar to logistic regression but can be used for more than just binary classification. Obviously, the first thing to do is to write out the formula:\n",
        "> $$a_j = \\frac{e^{z_j}}{\\sum_{k=1}^C e^{z_k}} = P(y = j | X)$$\n",
        "> where:\n",
        "- $e$ is euler's number $(≈2.71828...)$\n",
        "- $C$ is the number of possible classes\n",
        "- $z_j = W_j⋅x + b_j$ which is the output of the previous FC layer\n",
        "- The denominator is the sum of the exponentials of all logits, ensuring that the output is normalized to 1\n",
        "\n",
        "In code, this is very easy:\n",
        "```\n",
        "exp_x = np.exp(x - np.max(x, axis=0, keepdims=True))\n",
        "return exp_x / np.sum(exp_x, axis=0, keepdims=True)\n",
        "```\n",
        "And that would be it for softmax. Our output will be the probability of all $C$ classes. In our case, $C=10$."
      ],
      "metadata": {
        "id": "vHADKFxtmjpk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Actual Softmax\n",
        "\n",
        "Be aware the actual implementaion of Softmax will be in the activation file since it still an activation function."
      ],
      "metadata": {
        "id": "sx8fRG7utDVd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def softmax(x):\n",
        "  '''\n",
        "  This function is compute the class probabilites for a certain data point\n",
        "  using softmax activation function\n",
        "  Input:\n",
        "    x: The input array of shape (batch_size, num_classes)\n",
        "  Output:\n",
        "    The class probabilites for the data point\n",
        "  '''\n",
        "\n",
        "  exp_x = np.exp(x - np.max(x, axis=0, keepdims=True))\n",
        "  return exp_x / np.sum(exp_x, axis=0, keepdims=True)"
      ],
      "metadata": {
        "id": "YxzDKnMdt7-d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Loss function**"
      ],
      "metadata": {
        "id": "VghZpe_pvbKl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Almost at the end of it all, now. After we've gotten the class probablities and made a decision by picking the one with the highest probability, we need to know whether our prediction is actually correct. This is a common thing even in standrad ML. To do this, we use a loss function that compares the predictions we've made against the true class. This loss function is meant to give our error rate/signal which is needed for optimization of our parameters. Remember the point of optimization is reducing a loss function to near 0.\n",
        "\n",
        "There are a lot of loss functions developed over the years:\n",
        "1. L1 loss (Mean Absolute Error)\n",
        "2. L2 loss (Mean Squared Error)\n",
        "3. Huber loss (Smooth Mean Absolute Error)\n",
        "4. Log-Cosh loss\n",
        "5. Quantile loss\n",
        "6. Cross-entroypy loss/ log loss\n",
        "7. Hinge loss\n",
        "\n",
        "Actually, the way I've arranged them is specific. The first 5 are loss function developed majorly for regression problems, while the last 2 were developed for classiification problem. Our task is a classification problem and so we'll stick to one of those 2. More specifically, we'll pick cross-entropy because it's the most popular."
      ],
      "metadata": {
        "id": "psGubgQRvfIu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Cross-entropy Loss"
      ],
      "metadata": {
        "id": "TCZwFjIIyE4O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Like I said, this is the most common loss function used when it comes to classification tasks. The reason is that it was developed to be more generally, unlike Hinge loss which was originally made for SVMs (Support Vector Machine). It is used when the output of classification model is between 0 and 1. The closer the predicted probability is closer to the correct class, the lower the loss is. We derive the formulas from the likelihood function:\n",
        "> $$L(θ|x) = f(x|θ) \\quad \\text{for } θ ∈ Θ$$\n",
        "\n",
        "Now there are 2 different implementations of this function:\n",
        "1. Binary cross-entropy: This is used when there are only 2 classes (binary classification)\n",
        "2. Standard cross-entropy: This is used when there are more than 2 classes (multiclass classification)\n",
        "\n",
        "I will write both implementations for the sake of fun:\n",
        "\n",
        "Binary cross-entropy:\n",
        "> $$L\\left(\\hat{y_i}, y_i\\right) = J(θ) = -\\frac{1}{m} \\sum_{i = 1}^m \\left(y_i⋅log(\\hat{y_i})+(1-y_i)·log(1-\\hat{y_i})\\right)$$\n",
        "> where:\n",
        "- $L = J(Θ)$ is the loss function\n",
        "- $θ$ are any parameters passing to the loss function such as weights and biases\n",
        "- $m$ is the size of the training examples (data point)\n",
        "- $y_i$ is the correct/true class of the $i$-th training example\n",
        "- $\\hat{y_i}$ is the predicted output by the model\n",
        "\n",
        ">This is the easier formation of the formula. It's easier to implement but sometimes you may come across other formulations such as:\n",
        ">> $$L\\left(\\hat{y_i}, y_i\\right) = J(θ)\n",
        "\\left\\{\n",
        "\\begin{array}{11}\n",
        "-\\text{log}(\\hat{y_i}), & \\text{if }\\quad y_i = 1\\\\\n",
        "-\\text{log}(1 - \\hat{y_1}), & \\text{if }\\quad y_i = 0\n",
        "\\end{array}\n",
        "\\right\\}$$\n",
        "Notations are retained.\n",
        "\n",
        "Cross-entropy:\n",
        "> $$L\\left(\\hat{y_i}, y_i\\right) = -\\frac{1}{m}\\sum_{i = 1}^m y_i ·log(\\hat{y_i})$$\n",
        "\n",
        "We will implement both because I love my life and like to make things simpler (just in case I want to change the use case for this project)."
      ],
      "metadata": {
        "id": "2ep7WSg8yKM1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Actual loss function"
      ],
      "metadata": {
        "id": "AAwk2qQ49gtX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "class loss():\n",
        "  def __init__(self, y_true, y_pred):\n",
        "    '''\n",
        "    Initialize the loss function.\n",
        "    Input:\n",
        "      y_true: The true labels.\n",
        "      y_pred: The predicted labels.\n",
        "    '''\n",
        "    self.m = y_true.shape[0]\n",
        "    self.y_true = y_true\n",
        "    self.y_pred = y_pred\n",
        "\n",
        "  def binary_cross_entropy(self):\n",
        "    '''\n",
        "    Compute the binary cross-entropy loss.\n",
        "    Output:\n",
        "      The binary cross-entropy loss.\n",
        "    '''\n",
        "    return -(1/self.m) * np.sum(self.y_true * np.log(self.y_pred) + (1 - self.y_true) * np.log(1 - self.y_pred))\n",
        "\n",
        "\n",
        "  def cross_entropy(self):\n",
        "    '''\n",
        "    Compute the cross-entropy loss for multi-class classification.\n",
        "    Output:\n",
        "      The cross-entropy loss.\n",
        "    '''\n",
        "    batch_size = self.y_pred.shape[0]\n",
        "    correct_class_probs = self.y_pred[np.arange(batch_size), self.y_true]\n",
        "    log_probs = np.log(correct_class_probs + 1e-15)\n",
        "    return -np.mean(log_probs)"
      ],
      "metadata": {
        "id": "k6sNjmdw9kOe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#as usual, test whether you've done bs\n",
        "# Example usage with dummy data\n",
        "logits = np.array([\n",
        "    [1.0, 2.0, 3.0, 6.0],\n",
        "    [0.5, 2.5, 2.0, 5.5],\n",
        "    [0.3, 2.7, 4.5, 6.1]\n",
        "])  # Logits of shape (3, 4) for a batch of 3 samples and 4 classes\n",
        "\n",
        "labels = np.array([3, 1, 2])  # True labels (class indices)\n",
        "\n",
        "# Step 1: Compute softmax probabilities\n",
        "probabilities = softmax(logits)\n",
        "print(\"Softmax Probabilities shape:\")\n",
        "print(probabilities.shape)\n",
        "\n",
        "# Step 2: Compute cross-entropy loss\n",
        "loss = loss(labels, probabilities)\n",
        "loss = loss.cross_entropy()\n",
        "\n",
        "print(\"Softmax Probabilities:\\n\", probabilities)\n",
        "print(\"Cross-Entropy Loss:\", loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gEmKjO9oIYh2",
        "outputId": "ac0ab744-9d14-4080-a422-ded82486be7f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Softmax Probabilities shape:\n",
            "(3, 4)\n",
            "Softmax Probabilities:\n",
            " [[0.47548496 0.21447841 0.17095278 0.36877214]\n",
            " [0.2883962  0.35361511 0.06289001 0.22367161]\n",
            " [0.23611884 0.43190648 0.76615721 0.40755625]]\n",
            "Cross-Entropy Loss: 0.7678301433356479\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Batch normalization**"
      ],
      "metadata": {
        "id": "gTauRMsQOqKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "At this point in time we have everything to implement a full CNN and a training loop. But we can still do some few other thing to make the network work better. One of the things we can do is by implementing batch normalization.\n",
        "\n",
        "A very nice quote that I heard from a Standford course, \"you want unit gaussian activations? Just make them so.\" This is a very good quote when you want to make your network better. Batch normalization does exactly what its name suggests and normalizes the output of a layer to ensure that they have a mean of 0 and a standard deviation of 1 during training. To do this, we need to compute the mean and variance for each feature in a mini-batch:\n",
        ">Emprical mean ($μ$): $$μ = \\frac{1}{m}\\sum_{i=1}^m x_i$$\n",
        ">Variance ($σ^2$): $$σ^2 = \\frac{1}{m}\\sum_{i=1}^m (x_i - μ)^2$$\n",
        "\n",
        "To normalize, simply subtract the mean and divide the standard deviation:\n",
        "> $$\\hat{x}_i = \\frac{x_i - μ}{\\sqrt{σ^2 + ϵ}}$$\n",
        "where:\n",
        " - $ϵ$ is some small positive number to avoid division by 0\n",
        "\n",
        "After normalizing, the network has to 'squash' the range of the output as well. It does this by scaling ($γ$) and shifting ($β$) using the formula:\n",
        "> $$y_k = γ_k⋅\\hat{x_k} + β_k$$\n",
        "\n",
        "All that is for the forward pass. For the back pass, we find the gradient of the loss w.r.t $\\hat{x_k}$, $γ$ and $β$\n",
        "> $$\\frac{∂L}{∂γ} = \\sum_{i =1}^k\\frac{∂L}{∂y_i} ⋅ \\hat{x_i}$$\n",
        "> $$\\frac{∂L}{∂β} = \\sum_{i =1}^k\\frac{∂L}{∂y_i}$$\n",
        "\n",
        "As usual, I will dervivation to the reader (just implement the chain rule and substitute). Let's see the code."
      ],
      "metadata": {
        "id": "SB5IwRBWO5Cu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Actual batch nomalization"
      ],
      "metadata": {
        "id": "tMPlKEZLW0eY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BatchNormalization:\n",
        "  def __init__(self, input_size, epsilon=1e-5, momentum=0.9):\n",
        "    '''\n",
        "    I really don't know what else I can say about initilizations.\n",
        "    Input:\n",
        "      input_size: batch size\n",
        "      epsilon: a small positive number to avoid division by 0\n",
        "      momentum: momentum coefficient (friction) when using SGD with momentum\n",
        "    '''\n",
        "    self.gamma = np.ones((input_size, 1))\n",
        "    self.beta = np.zeros((input_size, 1))\n",
        "    self.epsilon = epsilon\n",
        "    self.momentum = momentum\n",
        "    self.running_mean = np.zeros((input_size, 1))\n",
        "    self.running_var = np.zeros((input_size, 1))\n",
        "\n",
        "  def forward(self, x, training=True):\n",
        "    '''\n",
        "    This is the forward pass for batch normalization\n",
        "    Input:\n",
        "      training: a boolean telling us whether the network should learn gamma and beta or not\n",
        "    Output:\n",
        "      The normalized output\n",
        "    '''\n",
        "    if training:\n",
        "        self.mean = np.mean(x, axis=1, keepdims=True)\n",
        "        self.var = np.var(x, axis=1, keepdims=True)\n",
        "        self.x_hat = (x - self.mean) / np.sqrt(self.var + self.epsilon)\n",
        "\n",
        "        # Update running estimates\n",
        "        self.running_mean = self.momentum * self.running_mean + (1 - self.momentum) * self.mean\n",
        "        self.running_var = self.momentum * self.running_var + (1 - self.momentum) * self.var\n",
        "    else:\n",
        "        # Use running estimates for inference\n",
        "        self.x_hat = (x - self.running_mean) / np.sqrt(self.running_var + self.epsilon)\n",
        "\n",
        "    self.out = self.gamma * self.x_hat + self.beta\n",
        "    return self.out\n",
        "\n",
        "  def backward(self, dout):\n",
        "    '''\n",
        "    This function is meant to 'undo' the effect of the normalization by backprop\n",
        "    so as to compute the gradient of x\n",
        "    Input:\n",
        "      dout: the gradient of a later layer in the network\n",
        "\n",
        "    Output:\n",
        "      dx: the gradient of x backpropagated through the batch normalization\n",
        "    '''\n",
        "    m = dout.shape[1]\n",
        "\n",
        "    # Gradients w.r.t. gamma and beta\n",
        "    self.dgamma = np.sum(dout * self.x_hat, axis=1, keepdims=True)\n",
        "    self.dbeta = np.sum(dout, axis=1, keepdims=True)\n",
        "\n",
        "    # Backprop through normalization\n",
        "    dx_hat = dout * self.gamma\n",
        "    dvar = np.sum(dx_hat * (self.input - self.mean) * -0.5 * (self.var + self.epsilon)**-1.5, axis=1, keepdims=True)\n",
        "    dmean = np.sum(dx_hat * -1 / np.sqrt(self.var + self.epsilon), axis=1, keepdims=True) + dvar * np.mean(-2 * (self.input - self.mean), axis=1, keepdims=True)\n",
        "\n",
        "    dx = dx_hat / np.sqrt(self.var + self.epsilon) + dvar * 2 * (self.input - self.mean) / m + dmean / m\n",
        "    return dx\n"
      ],
      "metadata": {
        "id": "Gks_UM0_XF5Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Dropout regularization**"
      ],
      "metadata": {
        "id": "aUpuFjSDOti-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Well, this is another rather simple concept. From the title of this section, you can tell what this is. One of the major issues in AI/ML is the issue of overfitting where the model fits the training data so well that it can't generalise to new data. We are actually more likely to overfit with NNs considering how complex they are, meaning they are more likely to form even more complex relationships in the training data. That being said we have to deal with this overfitting issue.\n",
        "\n",
        "In ML, we use techniques such as L1 and L2 regularization where we add a regularization term to the loss function. Unfortunately, this doesn't work as well with NNs. For this, we use a very simple concept called dropout. This is where we randomly 'kill' neurons but setting them to 0.\n",
        "\n",
        "During training, we generate a binary mask $M$ with a keep probabilty $p$\n",
        "> $$M \\sim \\text{Bernoulli}(p)$$\n",
        "and then multiply the input by the mask\n",
        "$$\\text{output} = M ⊙ x$$"
      ],
      "metadata": {
        "id": "mJ-ohQytedk9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Dropout:\n",
        "    def __init__(self, keep_prob=0.5):\n",
        "        self.keep_prob = keep_prob\n",
        "\n",
        "    def forward(self, x, training=True):\n",
        "        if training:\n",
        "            self.mask = (np.random.rand(*x.shape) < self.keep_prob) / self.keep_prob\n",
        "            return x * self.mask\n",
        "        else:\n",
        "            return x  # During inference, no dropout is applied\n",
        "\n",
        "    def backward(self, dout):\n",
        "        return dout * self.mask\n"
      ],
      "metadata": {
        "id": "aSAtxpt0ec7k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Evaluation Metrics**"
      ],
      "metadata": {
        "id": "xRQ7cHGvgYx1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Considering that we have everything that would be needed to build the CNN, we still need to know whether our CNN is actually performing well. Yes, the loss can tell us that, but that's majorly for the sake of learning in training. We want to evaluate the network after we have trained and on testing. To do this, there are a lot of ways that are common in both standard ML and here in DL. For that reason, I'll go straight to it.\n",
        "\n",
        "For this project, I'll be using accuracy (the simplest evaluation metric ever) and a confusion matrix. The first is so easy that I'll just give the formula and you can see why:\n",
        "> $$\\text{Accuracy} = \\frac{\\text{Number of Correct Prediction}}{\\text{Total Number of Predictions}}$$\n",
        "Like I said, easy.\n",
        "\n",
        "The confusion matrix may be in the relatively tricky one. It's difficult at all to understand. It's just a bit more verbose than accuracy. The point of a confusion matrix is to have a matrix of shape $C$ x $C$ where $C$ is the number of classes that shows the relation of between true class labels and predicted classes such that the leading diagonal represents the number of correctly predicted labels. To put it somewhat mathematically:\n",
        "> $$\\text{Confusion Matrix}[i,j] = \\text{Number of times class }i \\text{ is predicted as class }j$$\n",
        "\n",
        "Can be easily and quickly implemented in code as follows:"
      ],
      "metadata": {
        "id": "qhW4z5r4geoF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Actual evaluation metrics"
      ],
      "metadata": {
        "id": "i3ZEFgUvjh0A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class evaluation():\n",
        "  def __init__(self, y_true, y_pred):\n",
        "    '''\n",
        "    Initialize the evaluation metrics.\n",
        "    Input:\n",
        "      y_true: The true class labels.\n",
        "      y_pred: The predicted labels by our network.\n",
        "    '''\n",
        "    self.y_true = y_true\n",
        "    self.y_pred = y_pred\n",
        "\n",
        "  def accuracy(self):\n",
        "    '''\n",
        "    Compute the accuracy of the model.\n",
        "    Output:\n",
        "      The accuracy of the model.\n",
        "    '''\n",
        "    return np.mean(self.y_true == self.y_pred)\n",
        "\n",
        "  def confusion_matrix(self, num_classes):\n",
        "    '''\n",
        "    Create a confusion matrix to show the relationship between classes.\n",
        "    Output:\n",
        "      The confusion matrix.\n",
        "    '''\n",
        "    conf_matrix = np.zeros((num_classes, num_classes), dtype=int)\n",
        "    for true_label, pred_label in zip(self.y_true, self.y_pred):\n",
        "        conf_matrix[true_label, pred_label] += 1\n",
        "    return conf_matrix"
      ],
      "metadata": {
        "id": "K0OzUPRpjhPo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Let's put it together now**"
      ],
      "metadata": {
        "id": "RyhDgkM4q-Rf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, that we've implemented everything that we need to implement from scratch, we can start using them to build our fully functional CNN. We'll do this before implementing the training loop, so we can have the network architecture/design first.\n",
        "\n",
        "So our architecture will look like this:\n",
        "> Conv2D layer $⟶$ MaxPool2D layer $⟶$ Conv2D layer $⟶$ MaxPool2D layer $⟶$ Flattening layer $⟶$ FC1 layer $⟶$ FC2 layer\n",
        "\n",
        "To be a bit more specific:\n",
        "1. Conv2D layers have the following specs:\n",
        " - Filters: 32\n",
        " - Filter shape: $3$ x $3$\n",
        " - Activation: PReLU\n",
        "\n",
        "2. Pooling Layer:\n",
        " - Window size: $2$ x $2$\n",
        "\n",
        "3. Fully Connected Layer 1 (FC1):\n",
        " - Neurons: 128\n",
        " - Activation: PReLU\n",
        " - Dropout: 0.5\n",
        "\n",
        "4. Fully Connected Layer 2 (FC2):\n",
        " - Neurons: 10\n",
        " - Activation: Softmax\n",
        "\n",
        " I'll use a class-based approach to simplify my life a bit."
      ],
      "metadata": {
        "id": "YElr8vhvrJGk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN():\n",
        "  def __init__(self):\n",
        "    self.conv1 = Conv2DLayer(input_shape=(1,28,28), filter_shape = (32,1,3,3),stride=2, padding=2)\n",
        "    self.pool1 = MaxPool2D(pool_size=(2,2), stride=2, padding=0)\n",
        "    self.conv2 = Conv2DLayer(input_shape=(32,14,14), filter_shape = (64,32,3,3),stride=2, padding=2)\n",
        "    self.pool2 = MaxPool2D(pool_size=(2,2), stride=2, padding=0)\n",
        "    self.flatten = Flatten()\n",
        "    self.fc1 = FullyConnectedLayer(1024, 128, activation_function=PReLU().forward, activation_derivative=PReLU().backward)\n",
        "    self.dropout = Dropout(keep_prob=0.5)\n",
        "    self.fc2 = FullyConnectedLayer(128, 10, activation_function=softmax().softmax, activation_derivative=None)\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = self.conv1.forward(X)\n",
        "    out = PReLU.forward(out)\n",
        "    out = self.pool1.forward(out)\n",
        "    out = self.conv2.forward(out)\n",
        "    out = PReLU.forward(out)\n",
        "    out = self.pool2.forward(out)\n",
        "    out = self.flatten.forward(out)\n",
        "    out = self.fc1.forward(out)\n",
        "    out = PReLU.forward(out)\n",
        "    out = self.dropout.forward(out)\n",
        "    out = self.fc2.forward(out)\n",
        "    out = self.softmax.forward(out)\n",
        "    return out\n",
        "\n",
        "  def backward(self, d_loss, learning_rate):\n",
        "    grad = self.softmax.backward(d_loss)\n",
        "    grad = self.fc2.backward(grad, learning_rate)\n",
        "    grad = self.dropout.backward(grad)\n",
        "    grad = PReLU.backward(grad)\n",
        "    grad = self.fc1.backward(grad, learning_rate)\n",
        "    grad = self.flatten.backward(grad)\n",
        "    grad = self.pool2.backward(grad)\n",
        "    grad = PReLU.backward(grad)\n",
        "    grad = self.conv2.backward(grad, learning_rate)\n",
        "    grad = self.pool1.backward(grad)\n",
        "    grad = PReLU.backward(grad)\n",
        "    grad = self.conv1.backward(grad, learning_rate)\n",
        "    return grad"
      ],
      "metadata": {
        "id": "ZUsK5bFuo0fj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the network\n",
        "cnn = CNN()\n",
        "\n",
        "# Training parameters\n",
        "epochs = 10\n",
        "batch_size = 64\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    permutation = np.random.permutation(len(X_train))\n",
        "    X_train_shuffled = X_train[permutation]\n",
        "    y_train_shuffled = y_train[permutation]\n",
        "\n",
        "    epoch_loss = 0\n",
        "    for i in range(0, len(X_train), batch_size):\n",
        "        # Mini-batch\n",
        "        X_batch = X_train_shuffled[i:i + batch_size]\n",
        "        y_batch = y_train_shuffled[i:i + batch_size]\n",
        "\n",
        "        # Forward pass\n",
        "        predictions = cnn.forward(X_batch)\n",
        "\n",
        "        # Compute loss and accuracy\n",
        "        loss = cross_entropy_loss(predictions, y_batch)\n",
        "        epoch_loss += loss\n",
        "\n",
        "        # Backward pass\n",
        "        d_loss = cross_entropy_loss_derivative(predictions, y_batch)\n",
        "        cnn.backward(d_loss, learning_rate)\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}/{epochs}, Loss: {epoch_loss / len(X_train)}\")\n"
      ],
      "metadata": {
        "id": "tLNWWq1qy830"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Combining these into a single and proper CNN will be done in another notebook that is all code (MNIST_full). This cookbook was majorly for explanations on the concepts used to implement the CNN."
      ],
      "metadata": {
        "id": "U9UMUdst24UL"
      }
    }
  ]
}